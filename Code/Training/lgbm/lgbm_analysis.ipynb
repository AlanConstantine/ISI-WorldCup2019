{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lgbm_analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-z2Oa-RDS4fy",
        "LWs2rSKXS4f4",
        "knY2oMcMS4f9",
        "EjXw600fS4gA",
        "l2xXiw3DS4gD",
        "f0LZkwvUS4gI",
        "kc8ydOamYXac"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:12:48.443319Z",
          "start_time": "2019-04-12T17:12:48.430799Z"
        },
        "id": "FkDMpgK2S4fd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import numpy as np\n",
        "# import modin.pandas as pd\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.feature_selection import RFE, f_regression\n",
        "\n",
        "import itertools\n",
        "\n",
        "import warnings\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "sns.set_style({'font.sans-serif': ['simsun', 'Arial']})\n",
        "sns.set_style('darkgrid', {'font.sans-serif': ['simhei', 'Arial']})\n",
        "%matplotlib inline\n",
        "\n",
        "# np.random.seed(4590)\n",
        "nf_data_path = r'./noFinacialFeatures.csv'\n",
        "f_data_path = r'./FinacialFeatures.csv'\n",
        "\n",
        "flevel = json.load(open(r'./feature_level.json'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:12:50.818441Z",
          "start_time": "2019-04-12T17:12:50.531675Z"
        },
        "id": "k4UGyHQiS4fh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nf_df = pd.read_csv(nf_data_path)\n",
        "f_df = pd.read_csv(f_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:12:59.083657Z",
          "start_time": "2019-04-12T17:12:59.031376Z"
        },
        "id": "XSO9F2ZOS4fk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df = nf_df.copy()\n",
        "f_df.drop(['企业总评分'], axis=1, inplace=True)\n",
        "df = df.merge(f_df, how='left', on='企业编号')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:13:03.856513Z",
          "start_time": "2019-04-12T17:13:03.840698Z"
        },
        "id": "08pXxQFlS4fp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = df[['企业编号', '企业总评分']]\n",
        "x = df.drop(['企业总评分'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:13:05.961591Z",
          "start_time": "2019-04-12T17:13:05.946308Z"
        },
        "scrolled": false,
        "id": "XAU1j-EfS4fr",
        "colab_type": "code",
        "outputId": "aa8ef8a3-109d-4886-f486-45e5c862d78a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=0)\n",
        "ytrain_id = ytrain['企业编号']\n",
        "ytrain = ytrain['企业总评分']\n",
        "ytest_id = ytest['企业编号']\n",
        "ytest = ytest['企业总评分']\n",
        "print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)\n",
        "\n",
        "id_train = xtrain['企业编号']\n",
        "id_test = xtest['企业编号']\n",
        "xtrain.drop(['企业编号'], axis=1, inplace=True)\n",
        "xtest.drop(['企业编号'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2364, 307) (592, 307) (2364,) (592,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:08:32.256905Z",
          "start_time": "2019-04-12T18:08:32.253210Z"
        },
        "id": "J5OJgH69S4fw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_(gs):\n",
        "    print(gs.cv_results_)\n",
        "    print(gs.best_params_)\n",
        "    print(gs.best_score_)\n",
        "    print('rmse:', np.sqrt(np.abs(gs.best_score_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMWNOT6Crh4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 调参"
      ]
    },
    {
      "metadata": {
        "id": "-z2Oa-RDS4fy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 学习率和估计器及其数目"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:18:32.960847Z",
          "start_time": "2019-04-12T17:18:32.956915Z"
        },
        "id": "n2jxuHclS4fz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = {'boosting_type': 'gbdt',\n",
        "          'objective': 'regression',\n",
        "\n",
        "          'learning_rate': 0.001,\n",
        "          'num_leaves': 110,\n",
        "          'max_depth': 7,\n",
        "\n",
        "          'subsample': 0.8,\n",
        "          'colsample_bytree': 0.8,\n",
        "          \"metric\": 'rmse', }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T17:20:49.303062Z",
          "start_time": "2019-04-12T17:19:12.352556Z"
        },
        "id": "q3NzwiJGS4f1",
        "colab_type": "code",
        "outputId": "3d0fc4e6-fc52-487c-833d-595547209ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1530
        }
      },
      "cell_type": "code",
      "source": [
        "data_train = lgb.Dataset(xtrain, ytrain, silent=True)\n",
        "cv_results = lgb.cv(\n",
        "    params, data_train, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='rmse',\n",
        "    early_stopping_rounds=50, verbose_eval=50, show_stdv=True, seed=0)\n",
        "\n",
        "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
        "print('best cv score:', cv_results['rmse-mean'][-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[50]\tcv_agg's rmse: 4.94681 + 0.159085\n",
            "[100]\tcv_agg's rmse: 4.82183 + 0.160349\n",
            "[150]\tcv_agg's rmse: 4.70485 + 0.161866\n",
            "[200]\tcv_agg's rmse: 4.59534 + 0.163504\n",
            "[250]\tcv_agg's rmse: 4.49212 + 0.165191\n",
            "[300]\tcv_agg's rmse: 4.39565 + 0.166612\n",
            "[350]\tcv_agg's rmse: 4.30632 + 0.167825\n",
            "[400]\tcv_agg's rmse: 4.22476 + 0.169009\n",
            "[450]\tcv_agg's rmse: 4.14814 + 0.169685\n",
            "[500]\tcv_agg's rmse: 4.07666 + 0.169847\n",
            "[550]\tcv_agg's rmse: 4.01103 + 0.170299\n",
            "[600]\tcv_agg's rmse: 3.94945 + 0.170854\n",
            "[650]\tcv_agg's rmse: 3.89271 + 0.171538\n",
            "[700]\tcv_agg's rmse: 3.84102 + 0.172256\n",
            "[750]\tcv_agg's rmse: 3.79258 + 0.17284\n",
            "[800]\tcv_agg's rmse: 3.74763 + 0.173087\n",
            "[850]\tcv_agg's rmse: 3.7063 + 0.173007\n",
            "[900]\tcv_agg's rmse: 3.66813 + 0.173361\n",
            "[950]\tcv_agg's rmse: 3.63244 + 0.172179\n",
            "[1000]\tcv_agg's rmse: 3.59988 + 0.171469\n",
            "[1050]\tcv_agg's rmse: 3.56919 + 0.170973\n",
            "[1100]\tcv_agg's rmse: 3.54149 + 0.170208\n",
            "[1150]\tcv_agg's rmse: 3.516 + 0.169106\n",
            "[1200]\tcv_agg's rmse: 3.49263 + 0.168392\n",
            "[1250]\tcv_agg's rmse: 3.47044 + 0.167392\n",
            "[1300]\tcv_agg's rmse: 3.4504 + 0.166621\n",
            "[1350]\tcv_agg's rmse: 3.43134 + 0.165741\n",
            "[1400]\tcv_agg's rmse: 3.41403 + 0.164613\n",
            "[1450]\tcv_agg's rmse: 3.39741 + 0.163155\n",
            "[1500]\tcv_agg's rmse: 3.38195 + 0.16173\n",
            "[1550]\tcv_agg's rmse: 3.3676 + 0.160338\n",
            "[1600]\tcv_agg's rmse: 3.35452 + 0.159606\n",
            "[1650]\tcv_agg's rmse: 3.34222 + 0.159095\n",
            "[1700]\tcv_agg's rmse: 3.33073 + 0.158273\n",
            "[1750]\tcv_agg's rmse: 3.32013 + 0.15779\n",
            "[1800]\tcv_agg's rmse: 3.31041 + 0.156745\n",
            "[1850]\tcv_agg's rmse: 3.30178 + 0.156209\n",
            "[1900]\tcv_agg's rmse: 3.29304 + 0.155755\n",
            "[1950]\tcv_agg's rmse: 3.28479 + 0.155476\n",
            "[2000]\tcv_agg's rmse: 3.27715 + 0.154938\n",
            "[2050]\tcv_agg's rmse: 3.27021 + 0.154319\n",
            "[2100]\tcv_agg's rmse: 3.26342 + 0.153717\n",
            "[2150]\tcv_agg's rmse: 3.25724 + 0.153132\n",
            "[2200]\tcv_agg's rmse: 3.25128 + 0.152629\n",
            "[2250]\tcv_agg's rmse: 3.24577 + 0.152332\n",
            "[2300]\tcv_agg's rmse: 3.24049 + 0.152001\n",
            "[2350]\tcv_agg's rmse: 3.23545 + 0.151584\n",
            "[2400]\tcv_agg's rmse: 3.23052 + 0.150971\n",
            "[2450]\tcv_agg's rmse: 3.22599 + 0.150282\n",
            "[2500]\tcv_agg's rmse: 3.22208 + 0.149335\n",
            "[2550]\tcv_agg's rmse: 3.21832 + 0.148497\n",
            "[2600]\tcv_agg's rmse: 3.21524 + 0.147774\n",
            "[2650]\tcv_agg's rmse: 3.21205 + 0.147393\n",
            "[2700]\tcv_agg's rmse: 3.20934 + 0.147019\n",
            "[2750]\tcv_agg's rmse: 3.2067 + 0.146622\n",
            "[2800]\tcv_agg's rmse: 3.20396 + 0.146028\n",
            "[2850]\tcv_agg's rmse: 3.20151 + 0.145722\n",
            "[2900]\tcv_agg's rmse: 3.19896 + 0.145053\n",
            "[2950]\tcv_agg's rmse: 3.1967 + 0.144598\n",
            "[3000]\tcv_agg's rmse: 3.19461 + 0.144134\n",
            "[3050]\tcv_agg's rmse: 3.19272 + 0.143609\n",
            "[3100]\tcv_agg's rmse: 3.19085 + 0.143241\n",
            "[3150]\tcv_agg's rmse: 3.18922 + 0.14283\n",
            "[3200]\tcv_agg's rmse: 3.18758 + 0.142518\n",
            "[3250]\tcv_agg's rmse: 3.18604 + 0.142004\n",
            "[3300]\tcv_agg's rmse: 3.18465 + 0.141384\n",
            "[3350]\tcv_agg's rmse: 3.1832 + 0.140852\n",
            "[3400]\tcv_agg's rmse: 3.1818 + 0.140316\n",
            "[3450]\tcv_agg's rmse: 3.18042 + 0.139841\n",
            "[3500]\tcv_agg's rmse: 3.17916 + 0.139416\n",
            "[3550]\tcv_agg's rmse: 3.17812 + 0.138794\n",
            "[3600]\tcv_agg's rmse: 3.17719 + 0.138146\n",
            "[3650]\tcv_agg's rmse: 3.17639 + 0.137738\n",
            "[3700]\tcv_agg's rmse: 3.17567 + 0.137337\n",
            "[3750]\tcv_agg's rmse: 3.17501 + 0.137259\n",
            "[3800]\tcv_agg's rmse: 3.17452 + 0.137108\n",
            "[3850]\tcv_agg's rmse: 3.17401 + 0.136778\n",
            "[3900]\tcv_agg's rmse: 3.1734 + 0.136373\n",
            "[3950]\tcv_agg's rmse: 3.17278 + 0.136031\n",
            "[4000]\tcv_agg's rmse: 3.17228 + 0.135832\n",
            "[4050]\tcv_agg's rmse: 3.17194 + 0.135552\n",
            "[4100]\tcv_agg's rmse: 3.17161 + 0.135385\n",
            "[4150]\tcv_agg's rmse: 3.17129 + 0.135023\n",
            "[4200]\tcv_agg's rmse: 3.17099 + 0.134775\n",
            "[4250]\tcv_agg's rmse: 3.17078 + 0.134594\n",
            "[4300]\tcv_agg's rmse: 3.17062 + 0.134457\n",
            "[4350]\tcv_agg's rmse: 3.17062 + 0.134288\n",
            "best n_estimators: 4339\n",
            "best cv score: 3.170522419394491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LWs2rSKXS4f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## max_depth 和 num_leaves"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:03:30.762447Z",
          "start_time": "2019-04-12T18:01:54.525920Z"
        },
        "id": "yLgPfBRqS4f5",
        "colab_type": "code",
        "outputId": "b5394004-afcc-4053-e3e5-e775a73610a9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_lgb = lgb.LGBMRegressor(objective='regression', num_leaves=50,\n",
        "                              learning_rate=0.1, n_estimators=43, max_depth=6,\n",
        "                              metric='rmse', bagging_fraction=0.8, feature_fraction=0.8)\n",
        "\n",
        "params_test1 = {\n",
        "    'max_depth': range(3, 8, 2),\n",
        "    'num_leaves': range(20, 100, 30)\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator=model_lgb, param_grid=params_test1,\n",
        "                        scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=2)\n",
        "gsearch1.fit(xtrain, ytrain)\n",
        "# print(gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_)\n",
        "# print('rmse:', np.sqrt(np.abs(gsearch1.best_score_)))\n",
        "print_(gsearch1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   32.9s\n",
            "[Parallel(n_jobs=2)]: Done  90 out of  90 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'mean_fit_time': array([0.92085559, 0.95250058, 0.81514375, 1.7938025 , 1.6340225 ,\n",
            "       1.4990643 , 1.79567962, 2.53424389, 2.49483378]), 'std_fit_time': array([0.00707319, 0.048489  , 0.1618288 , 0.20759863, 0.29997807,\n",
            "       0.26141865, 0.29782629, 0.54207498, 0.4510752 ]), 'mean_score_time': array([0.10262785, 0.09872396, 0.07656128, 0.09280767, 0.07676151,\n",
            "       0.0744283 , 0.078845  , 0.07643392, 0.07436543]), 'std_score_time': array([0.00187215, 0.01305794, 0.01944329, 0.01111545, 0.01643953,\n",
            "       0.01687405, 0.02342747, 0.01855839, 0.01470031]), 'param_max_depth': masked_array(data=[3, 3, 3, 5, 5, 5, 7, 7, 7],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_num_leaves': masked_array(data=[20, 50, 80, 20, 50, 80, 20, 50, 80],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'max_depth': 3, 'num_leaves': 20}, {'max_depth': 3, 'num_leaves': 50}, {'max_depth': 3, 'num_leaves': 80}, {'max_depth': 5, 'num_leaves': 20}, {'max_depth': 5, 'num_leaves': 50}, {'max_depth': 5, 'num_leaves': 80}, {'max_depth': 7, 'num_leaves': 20}, {'max_depth': 7, 'num_leaves': 50}, {'max_depth': 7, 'num_leaves': 80}], 'split0_test_score': array([-9.65796381, -9.65796381, -9.65796381, -9.13642369, -9.3788825 ,\n",
            "       -9.3788825 , -9.53930807, -9.43953439, -9.642875  ]), 'split1_test_score': array([-9.62467819, -9.62467819, -9.62467819, -9.1901026 , -9.37162607,\n",
            "       -9.37162607, -9.63046437, -9.5595166 , -9.60667178]), 'split2_test_score': array([-10.22060325, -10.22060325, -10.22060325,  -9.7679317 ,\n",
            "        -9.77492222,  -9.77492222,  -9.93192218, -10.10626348,\n",
            "       -10.01272905]), 'split3_test_score': array([-12.78584686, -12.78584686, -12.78584686, -13.14618876,\n",
            "       -13.05704811, -13.05704811, -12.68911417, -13.02532639,\n",
            "       -13.14633591]), 'split4_test_score': array([-9.42983494, -9.42983494, -9.42983494, -9.27513144, -9.31656975,\n",
            "       -9.31656975, -8.75009966, -9.64369703, -9.57631139]), 'split5_test_score': array([-10.70277377, -10.70277377, -10.70277377, -10.31781605,\n",
            "       -10.42771766, -10.42771766, -10.33178033, -10.36328631,\n",
            "       -10.30426418]), 'split6_test_score': array([ -9.91021816,  -9.91021816,  -9.91021816, -10.30451018,\n",
            "       -10.04024459, -10.04024459, -10.21306081,  -9.95929287,\n",
            "       -10.32812883]), 'split7_test_score': array([-11.11020786, -11.11020786, -11.11020786, -10.45489124,\n",
            "       -10.7412205 , -10.7412205 , -10.98114996, -11.05398686,\n",
            "       -10.48097811]), 'split8_test_score': array([-10.3443143 , -10.3443143 , -10.3443143 ,  -9.95418585,\n",
            "        -9.89289559,  -9.89289559, -10.00048931, -10.07163281,\n",
            "       -10.48857413]), 'split9_test_score': array([-10.51803924, -10.51803924, -10.51803924, -10.36188096,\n",
            "       -10.40141959, -10.40141959, -10.2291672 , -11.03050547,\n",
            "       -11.08087413]), 'mean_test_score': array([-10.43068801, -10.43068801, -10.43068801, -10.19110803,\n",
            "       -10.24051754, -10.24051754, -10.23002455, -10.42548587,\n",
            "       -10.46700332]), 'std_test_score': array([0.9314181 , 0.9314181 , 0.9314181 , 1.09886352, 1.04859516,\n",
            "       1.04859516, 0.99010978, 1.01590481, 1.00206958]), 'rank_test_score': array([6, 6, 6, 1, 3, 3, 2, 5, 9], dtype=int32), 'split0_train_score': array([-8.06504099, -8.06504099, -8.06504099, -5.02633972, -4.58964759,\n",
            "       -4.58964759, -4.51719665, -2.54886892, -2.62812941]), 'split1_train_score': array([-8.20734527, -8.20734527, -8.20734527, -4.91945581, -4.6919806 ,\n",
            "       -4.6919806 , -4.49102143, -2.55193818, -2.47617316]), 'split2_train_score': array([-8.09971078, -8.09971078, -8.09971078, -5.04418844, -4.59963011,\n",
            "       -4.59963011, -4.42978865, -2.43250958, -2.41171325]), 'split3_train_score': array([-7.79454806, -7.79454806, -7.79454806, -4.82710606, -4.61790248,\n",
            "       -4.61790248, -4.22516822, -2.52679267, -2.51992301]), 'split4_train_score': array([-8.14078186, -8.14078186, -8.14078186, -4.93060868, -4.7280871 ,\n",
            "       -4.7280871 , -4.46421463, -2.41944511, -2.48393655]), 'split5_train_score': array([-8.0527094 , -8.0527094 , -8.0527094 , -4.89307038, -4.61989473,\n",
            "       -4.61989473, -4.35630527, -2.54995573, -2.56500731]), 'split6_train_score': array([-8.03955544, -8.03955544, -8.03955544, -4.90495731, -4.63418401,\n",
            "       -4.63418401, -4.36960227, -2.56255411, -2.40073858]), 'split7_train_score': array([-7.9217967 , -7.9217967 , -7.9217967 , -4.96863479, -4.63624356,\n",
            "       -4.63624356, -4.41185454, -2.5415296 , -2.5826893 ]), 'split8_train_score': array([-8.09398847, -8.09398847, -8.09398847, -5.01404912, -4.72932862,\n",
            "       -4.72932862, -4.39890238, -2.57564647, -2.54527427]), 'split9_train_score': array([-8.12885428, -8.12885428, -8.12885428, -5.01385112, -4.75450726,\n",
            "       -4.75450726, -4.40533084, -2.58816431, -2.52502661]), 'mean_train_score': array([-8.05443313, -8.05443313, -8.05443313, -4.95422614, -4.66014061,\n",
            "       -4.66014061, -4.40693849, -2.52974047, -2.51386115]), 'std_train_score': array([0.11200935, 0.11200935, 0.11200935, 0.06687962, 0.05709812,\n",
            "       0.05709812, 0.07753642, 0.05443466, 0.06863341])} {'max_depth': 5, 'num_leaves': 20} -10.191108033098851\n",
            "rmse: 3.192351489591779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:08:31.743077Z",
          "start_time": "2019-04-12T18:05:45.373904Z"
        },
        "id": "nToXxkxmS4f7",
        "colab_type": "code",
        "outputId": "e6964379-ea9b-4a8a-ba53-c846217564be",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params_test2 = {\n",
        "    'max_depth': [4, 5, 6],\n",
        "    'num_leaves': [14, 16, 18, 20, 22, 24]\n",
        "}\n",
        "\n",
        "gsearch2 = GridSearchCV(estimator=model_lgb, param_grid=params_test2,\n",
        "                        scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=2)\n",
        "gsearch2.fit(xtrain, ytrain)\n",
        "print(gsearch2.cv_results_)\n",
        "print(gsearch2.best_params_)\n",
        "print(gsearch2.best_score_)\n",
        "print('rmse:', np.sqrt(np.abs(gsearch2.best_score_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   30.3s\n",
            "[Parallel(n_jobs=2)]: Done 180 out of 180 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'mean_fit_time': array([1.36653383, 1.09408345, 1.03487799, 1.03452284, 1.07209079,\n",
            "       1.03925378, 1.26209984, 1.32340829, 1.38175254, 1.73965721,\n",
            "       1.89382095, 1.86431124, 1.78585267, 1.89364927, 1.93473694,\n",
            "       1.71859808, 1.77160425, 2.18911905]), 'std_fit_time': array([0.04607406, 0.21578606, 0.17471289, 0.18349589, 0.18418632,\n",
            "       0.18045577, 0.22674089, 0.23940779, 0.25423431, 0.25700037,\n",
            "       0.07969668, 0.04767033, 0.01752817, 0.03163411, 0.28548189,\n",
            "       0.36152996, 0.31908205, 0.48044758]), 'mean_score_time': array([0.10029099, 0.07462955, 0.07196858, 0.07183502, 0.07426641,\n",
            "       0.07207785, 0.07141411, 0.07167449, 0.07225733, 0.09043505,\n",
            "       0.09697895, 0.09515953, 0.09257777, 0.09314201, 0.09124854,\n",
            "       0.07639015, 0.0793395 , 0.08808327]), 'std_score_time': array([0.01312466, 0.01736638, 0.0132277 , 0.01328873, 0.01367796,\n",
            "       0.01359791, 0.0135743 , 0.01354999, 0.01431537, 0.01352738,\n",
            "       0.00437607, 0.00464146, 0.00053888, 0.00126583, 0.02143389,\n",
            "       0.01811016, 0.01838989, 0.02080177]), 'param_max_depth': masked_array(data=[4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_num_leaves': masked_array(data=[14, 16, 18, 20, 22, 24, 14, 16, 18, 20, 22, 24, 14, 16,\n",
            "                   18, 20, 22, 24],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'max_depth': 4, 'num_leaves': 14}, {'max_depth': 4, 'num_leaves': 16}, {'max_depth': 4, 'num_leaves': 18}, {'max_depth': 4, 'num_leaves': 20}, {'max_depth': 4, 'num_leaves': 22}, {'max_depth': 4, 'num_leaves': 24}, {'max_depth': 5, 'num_leaves': 14}, {'max_depth': 5, 'num_leaves': 16}, {'max_depth': 5, 'num_leaves': 18}, {'max_depth': 5, 'num_leaves': 20}, {'max_depth': 5, 'num_leaves': 22}, {'max_depth': 5, 'num_leaves': 24}, {'max_depth': 6, 'num_leaves': 14}, {'max_depth': 6, 'num_leaves': 16}, {'max_depth': 6, 'num_leaves': 18}, {'max_depth': 6, 'num_leaves': 20}, {'max_depth': 6, 'num_leaves': 22}, {'max_depth': 6, 'num_leaves': 24}], 'split0_test_score': array([-9.62726005, -9.68033448, -9.68033448, -9.68033448, -9.68033448,\n",
            "       -9.68033448, -9.27558101, -9.63774281, -9.33751208, -9.13642369,\n",
            "       -9.43924557, -9.18362922, -9.15240805, -9.01199   , -9.44528841,\n",
            "       -9.6169209 , -9.74957674, -9.68287469]), 'split1_test_score': array([-9.34716182, -9.37195866, -9.37195866, -9.37195866, -9.37195866,\n",
            "       -9.37195866, -9.57539944, -9.30396705, -9.4466259 , -9.1901026 ,\n",
            "       -9.5779687 , -9.43194253, -9.37737678, -9.39830226, -9.65012372,\n",
            "       -9.60424363, -9.66839819, -9.50040258]), 'split2_test_score': array([-10.0141408 ,  -9.78977255,  -9.78977255,  -9.78977255,\n",
            "        -9.78977255,  -9.78977255,  -9.87584313,  -9.87931457,\n",
            "        -9.71814187,  -9.7679317 ,  -9.65570867,  -9.94099056,\n",
            "       -10.131418  ,  -9.79997774,  -9.4793617 ,  -9.53540567,\n",
            "        -9.80620459,  -9.45584016]), 'split3_test_score': array([-12.95958262, -12.80088036, -12.80088036, -12.80088036,\n",
            "       -12.80088036, -12.80088036, -13.01520436, -12.95680956,\n",
            "       -12.7559526 , -13.14618876, -12.81884961, -13.0423489 ,\n",
            "       -12.73179615, -12.54367047, -12.73410033, -12.86310476,\n",
            "       -12.50596606, -12.99611123]), 'split4_test_score': array([-9.01933434, -8.9346861 , -8.9346861 , -8.9346861 , -8.9346861 ,\n",
            "       -8.9346861 , -8.89954696, -8.54862289, -9.12599894, -9.27513144,\n",
            "       -9.24838774, -9.15307281, -9.14950037, -8.68293409, -8.94066863,\n",
            "       -9.1499982 , -9.09521037, -9.40487311]), 'split5_test_score': array([-10.3264177 , -10.29412878, -10.29412878, -10.29412878,\n",
            "       -10.29412878, -10.29412878, -10.42054023, -10.39311336,\n",
            "       -10.72346483, -10.31781605, -10.35239918, -10.11774297,\n",
            "       -10.54533604, -10.65358641, -10.59358545, -10.5613567 ,\n",
            "       -10.60485485, -10.4391761 ]), 'split6_test_score': array([-10.03051271, -10.05026037, -10.05026037, -10.05026037,\n",
            "       -10.05026037, -10.05026037, -10.17865573,  -9.89782764,\n",
            "       -10.16233614, -10.30451018,  -9.98598274, -10.2251443 ,\n",
            "        -9.81080748, -10.14584682,  -9.98025267, -10.0479498 ,\n",
            "        -9.98687754,  -9.82853892]), 'split7_test_score': array([-11.07644943, -10.62734255, -10.62734255, -10.62734255,\n",
            "       -10.62734255, -10.62734255, -10.61498083, -10.65766952,\n",
            "       -10.56526356, -10.45489124, -11.06842596, -10.70705465,\n",
            "       -10.13889824, -10.23826491, -10.89811396, -11.14178538,\n",
            "       -10.27571674, -11.01924753]), 'split8_test_score': array([-10.22577066, -10.13437685, -10.13437685, -10.13437685,\n",
            "       -10.13437685, -10.13437685,  -9.84188243, -10.06172983,\n",
            "       -10.02727945,  -9.95418585, -10.29155606,  -9.77590699,\n",
            "       -10.14870252,  -9.84490258,  -9.76842436,  -9.81502755,\n",
            "       -10.00616249, -10.14960871]), 'split9_test_score': array([-10.67339746, -10.15537307, -10.15537307, -10.15537307,\n",
            "       -10.15537307, -10.15537307, -10.52864805, -10.30847176,\n",
            "       -10.3383156 , -10.36188096, -10.42550113, -10.2552726 ,\n",
            "       -10.30440253, -10.10681788, -10.41052052, -10.28118682,\n",
            "       -10.53987946, -10.26989998]), 'mean_test_score': array([-10.33026847, -10.18429518, -10.18429518, -10.18429518,\n",
            "       -10.18429518, -10.18429518, -10.22298842, -10.16500056,\n",
            "       -10.22024894, -10.19110803, -10.28654896, -10.18367674,\n",
            "       -10.14940165, -10.04287611, -10.19027608, -10.26194028,\n",
            "       -10.22423775, -10.27488429]), 'std_test_score': array([1.04809813, 0.98611687, 0.98611687, 0.98611687, 0.98611687,\n",
            "       0.98611687, 1.0695087 , 1.09191309, 0.98496216, 1.09886352,\n",
            "       0.99252031, 1.06451597, 0.97842846, 1.00605682, 1.01659398,\n",
            "       1.02495399, 0.86909768, 1.0295348 ]), 'rank_test_score': array([18,  5,  5,  5,  5,  5, 13,  3, 12, 11, 17,  4,  2,  1, 10, 15, 14,\n",
            "       16], dtype=int32), 'split0_train_score': array([-6.35691014, -6.29838543, -6.29838543, -6.29838543, -6.29838543,\n",
            "       -6.29838543, -5.92281835, -5.5078113 , -5.12407118, -5.02633972,\n",
            "       -4.81757565, -4.7155435 , -5.74730211, -5.32605685, -4.90832314,\n",
            "       -4.56896201, -4.3636864 , -4.10682386]), 'split1_train_score': array([-6.49934733, -6.34177092, -6.34177092, -6.34177092, -6.34177092,\n",
            "       -6.34177092, -5.86032616, -5.47818331, -5.1326589 , -4.91945581,\n",
            "       -4.7480804 , -4.67079999, -5.72659045, -5.30090493, -4.93763197,\n",
            "       -4.48233604, -4.22585519, -4.09547474]), 'split2_train_score': array([-6.37435734, -6.25950823, -6.25950823, -6.25950823, -6.25950823,\n",
            "       -6.25950823, -5.83210854, -5.4350366 , -5.27909589, -5.04418844,\n",
            "       -4.75163807, -4.68804267, -5.71301878, -5.24936188, -4.90867538,\n",
            "       -4.52408103, -4.35978703, -4.08623687]), 'split3_train_score': array([-6.17516281, -6.12868584, -6.12868584, -6.12868584, -6.12868584,\n",
            "       -6.12868584, -5.79510454, -5.35825757, -5.03044398, -4.82710606,\n",
            "       -4.68754345, -4.60670231, -5.5950407 , -5.08064512, -4.70027519,\n",
            "       -4.50858051, -4.10925997, -3.94827667]), 'split4_train_score': array([-6.43702807, -6.35446439, -6.35446439, -6.35446439, -6.35446439,\n",
            "       -6.35446439, -5.88687025, -5.50079791, -5.09906666, -4.93060868,\n",
            "       -4.82033873, -4.70880768, -5.76843824, -5.24060371, -4.93036249,\n",
            "       -4.58160218, -4.25854653, -4.08587797]), 'split5_train_score': array([-6.30045176, -6.19614162, -6.19614162, -6.19614162, -6.19614162,\n",
            "       -6.19614162, -5.74707952, -5.39451328, -5.05512134, -4.89307038,\n",
            "       -4.79353355, -4.62588381, -5.65227834, -5.1951979 , -4.8189576 ,\n",
            "       -4.51014864, -4.22580644, -3.94085058]), 'split6_train_score': array([-6.26675923, -6.20050101, -6.20050101, -6.20050101, -6.20050101,\n",
            "       -6.20050101, -5.84231092, -5.4113936 , -5.08908463, -4.90495731,\n",
            "       -4.75058791, -4.72741248, -5.69452309, -5.24109314, -4.79588014,\n",
            "       -4.63797558, -4.29476875, -3.98758223]), 'split7_train_score': array([-6.14898615, -6.2329048 , -6.2329048 , -6.2329048 , -6.2329048 ,\n",
            "       -6.2329048 , -5.68165555, -5.41453686, -5.00976602, -4.96863479,\n",
            "       -4.80347544, -4.63307325, -5.6222539 , -5.18281276, -4.84561793,\n",
            "       -4.5330188 , -4.30575726, -3.98995096]), 'split8_train_score': array([-6.26958921, -6.31577302, -6.31577302, -6.31577302, -6.31577302,\n",
            "       -6.31577302, -5.87049412, -5.47702462, -5.1808972 , -5.01404912,\n",
            "       -4.77283895, -4.7539118 , -5.72807176, -5.23024682, -4.82179205,\n",
            "       -4.51257563, -4.2285083 , -4.12751088]), 'split9_train_score': array([-6.35979692, -6.33245338, -6.33245338, -6.33245338, -6.33245338,\n",
            "       -6.33245338, -5.82378645, -5.43315908, -5.11218613, -5.01385112,\n",
            "       -4.75945167, -4.86671553, -5.7178896 , -5.29039496, -4.85566271,\n",
            "       -4.6493863 , -4.31103529, -4.14013997]), 'mean_train_score': array([-6.3188389 , -6.26605886, -6.26605886, -6.26605886, -6.26605886,\n",
            "       -6.26605886, -5.82625544, -5.44107141, -5.11123919, -4.95422614,\n",
            "       -4.77050638, -4.6996893 , -5.6965407 , -5.23373181, -4.85231786,\n",
            "       -4.55086667, -4.26830112, -4.05087247]), 'std_train_score': array([0.10379303, 0.07118447, 0.07118447, 0.07118447, 0.07118447,\n",
            "       0.07118447, 0.06675544, 0.04626128, 0.07366502, 0.06687962,\n",
            "       0.0381866 , 0.07172355, 0.0530608 , 0.06642127, 0.06934351,\n",
            "       0.05404715, 0.07209569, 0.07196241])}\n",
            "{'max_depth': 6, 'num_leaves': 16}\n",
            "-10.042876111068885\n",
            "rmse: 3.1690497173551706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "knY2oMcMS4f9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## min_data_in_leaf 和 min_sum_hessian_in_leaf"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:13:04.772370Z",
          "start_time": "2019-04-12T18:11:45.491907Z"
        },
        "id": "g_DU5J0eS4f-",
        "colab_type": "code",
        "outputId": "0a5922af-0f2c-42b0-a9cb-d6f45303f720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1201
        }
      },
      "cell_type": "code",
      "source": [
        "params_test3 = {\n",
        "    'min_data_in_leaf': [18, 19, 20, 21, 22],\n",
        "    'min_sum_hessian_in_leaf': [0.001, 0.002]\n",
        "}\n",
        "model_lgb = lgb.LGBMRegressor(objective='regression', num_leaves=100,\n",
        "                              learning_rate=0.1, n_estimators=941, max_depth=7,\n",
        "                              metric='rmse', bagging_fraction=0.7, feature_fraction=0.8)\n",
        "gsearch3 = GridSearchCV(estimator=model_lgb, param_grid=params_test3,\n",
        "                        scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=8)\n",
        "gsearch3.fit(xtrain, ytrain)\n",
        "print_(gsearch3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0a1e6f46e990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m gsearch3 = GridSearchCV(estimator=model_lgb, param_grid=params_test3,\n\u001b[1;32m      9\u001b[0m                         scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=8)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgsearch3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsearch3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "EjXw600fS4gA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## feature_fraction 和 bagging_fraction"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:20:16.740009Z",
          "start_time": "2019-04-12T18:17:27.994694Z"
        },
        "id": "xywvIYOPS4gA",
        "colab_type": "code",
        "outputId": "c7e9dbb6-5c87-4f9b-dc97-f0e71a3e07ae",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params_test4 = {\n",
        "    'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
        "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "}\n",
        "model_lgb = lgb.LGBMRegressor(objective='regression',\n",
        "                              num_leaves=16,\n",
        "                              learning_rate=0.1,\n",
        "                              n_estimators=43,\n",
        "                              max_depth=6,\n",
        "                              metric='rmse',\n",
        "                              bagging_freq=5,\n",
        "                              min_child_samples=20,\n",
        "                              min_child_weight=0.001)\n",
        "gsearch4 = GridSearchCV(estimator=model_lgb, param_grid=params_test4,\n",
        "                        scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
        "gsearch4.fit(xtrain, ytrain)\n",
        "print_(gsearch4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 25 candidates, totalling 250 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   25.6s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed:  2.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'mean_fit_time': array([1.52890165, 1.68892879, 1.99096341, 2.45279725, 2.96322303,\n",
            "       1.53853116, 1.81281204, 1.9841588 , 2.50049465, 2.72619491,\n",
            "       1.80894279, 2.02389319, 2.2905458 , 2.38861339, 2.62607818,\n",
            "       1.72504992, 1.91050556, 2.28192964, 2.60278957, 2.64744437,\n",
            "       1.66610479, 2.00411994, 2.25824735, 2.47829208, 2.62041569]), 'std_fit_time': array([0.1440184 , 0.13613726, 0.13121932, 0.31410123, 0.52506249,\n",
            "       0.24742629, 0.16123335, 0.18808681, 0.20924647, 0.20986292,\n",
            "       0.21038671, 0.2015985 , 0.21959279, 0.14541002, 0.16114324,\n",
            "       0.12275598, 0.10543616, 0.1872898 , 0.1722875 , 0.07166341,\n",
            "       0.15272759, 0.18114585, 0.16659856, 0.17745014, 0.22466804]), 'mean_score_time': array([0.12549422, 0.10039601, 0.10719349, 0.14675357, 0.14070678,\n",
            "       0.1203696 , 0.13885729, 0.11928558, 0.13347023, 0.14235203,\n",
            "       0.14960563, 0.13403726, 0.15306253, 0.12587166, 0.11638017,\n",
            "       0.12471328, 0.12208233, 0.1329752 , 0.12285206, 0.12949545,\n",
            "       0.14539132, 0.11759193, 0.12965195, 0.12067773, 0.10928082]), 'std_score_time': array([0.03200195, 0.01435213, 0.01716274, 0.02536522, 0.03911455,\n",
            "       0.04227749, 0.03610817, 0.02234126, 0.03353327, 0.03577686,\n",
            "       0.04225376, 0.03609683, 0.04534714, 0.02889114, 0.02902703,\n",
            "       0.02473203, 0.03168169, 0.03523456, 0.02656987, 0.03108454,\n",
            "       0.03558586, 0.02267596, 0.03439965, 0.03308435, 0.01827153]), 'param_bagging_fraction': masked_array(data=[0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.8,\n",
            "                   0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.9, 0.9, 1.0, 1.0,\n",
            "                   1.0, 1.0, 1.0],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_feature_fraction': masked_array(data=[0.5, 0.6, 0.7, 0.8, 0.9, 0.5, 0.6, 0.7, 0.8, 0.9, 0.5,\n",
            "                   0.6, 0.7, 0.8, 0.9, 0.5, 0.6, 0.7, 0.8, 0.9, 0.5, 0.6,\n",
            "                   0.7, 0.8, 0.9],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'bagging_fraction': 0.6, 'feature_fraction': 0.5}, {'bagging_fraction': 0.6, 'feature_fraction': 0.6}, {'bagging_fraction': 0.6, 'feature_fraction': 0.7}, {'bagging_fraction': 0.6, 'feature_fraction': 0.8}, {'bagging_fraction': 0.6, 'feature_fraction': 0.9}, {'bagging_fraction': 0.7, 'feature_fraction': 0.5}, {'bagging_fraction': 0.7, 'feature_fraction': 0.6}, {'bagging_fraction': 0.7, 'feature_fraction': 0.7}, {'bagging_fraction': 0.7, 'feature_fraction': 0.8}, {'bagging_fraction': 0.7, 'feature_fraction': 0.9}, {'bagging_fraction': 0.8, 'feature_fraction': 0.5}, {'bagging_fraction': 0.8, 'feature_fraction': 0.6}, {'bagging_fraction': 0.8, 'feature_fraction': 0.7}, {'bagging_fraction': 0.8, 'feature_fraction': 0.8}, {'bagging_fraction': 0.8, 'feature_fraction': 0.9}, {'bagging_fraction': 0.9, 'feature_fraction': 0.5}, {'bagging_fraction': 0.9, 'feature_fraction': 0.6}, {'bagging_fraction': 0.9, 'feature_fraction': 0.7}, {'bagging_fraction': 0.9, 'feature_fraction': 0.8}, {'bagging_fraction': 0.9, 'feature_fraction': 0.9}, {'bagging_fraction': 1.0, 'feature_fraction': 0.5}, {'bagging_fraction': 1.0, 'feature_fraction': 0.6}, {'bagging_fraction': 1.0, 'feature_fraction': 0.7}, {'bagging_fraction': 1.0, 'feature_fraction': 0.8}, {'bagging_fraction': 1.0, 'feature_fraction': 0.9}], 'split0_test_score': array([-9.55758557, -9.38277355, -9.10898534, -9.46558885, -9.8327881 ,\n",
            "       -9.44675656, -9.70711867, -9.34207234, -9.24462505, -9.44897092,\n",
            "       -9.27012281, -9.16863097, -9.35278388, -9.23090127, -9.0907351 ,\n",
            "       -9.17886691, -9.18797124, -9.40296437, -9.08956981, -8.98972717,\n",
            "       -9.45123253, -9.17339171, -9.2794338 , -9.01199   , -9.36972426]), 'split1_test_score': array([-9.53271739, -9.48116127, -9.17098993, -9.12678199, -9.33261134,\n",
            "       -9.48214855, -9.02279998, -9.0338957 , -8.90719977, -9.14122781,\n",
            "       -9.60990607, -9.02881551, -9.04544321, -9.10348027, -9.21239327,\n",
            "       -9.63952656, -9.53710596, -9.21440698, -9.08964062, -9.84295933,\n",
            "       -9.44504203, -9.14005385, -9.41071852, -9.39830226, -9.51429325]), 'split2_test_score': array([-10.1260182 ,  -9.62729173,  -9.40332694,  -9.70125532,\n",
            "        -9.37553976, -10.15891786,  -9.4736186 ,  -9.66511933,\n",
            "        -9.68480545,  -9.63515036,  -9.65590242,  -9.37136142,\n",
            "        -9.83524421,  -9.69002257,  -9.57651146,  -9.73131686,\n",
            "        -9.65717854,  -9.51838209,  -9.70409249,  -9.93118662,\n",
            "        -9.42998003,  -9.75910027,  -9.70214619,  -9.79997774,\n",
            "        -9.69837701]), 'split3_test_score': array([-13.32203658, -12.88477667, -12.8951238 , -12.69210592,\n",
            "       -13.08059385, -12.68717499, -12.44297104, -12.70873074,\n",
            "       -12.0824704 , -13.00994444, -13.09766943, -13.06767741,\n",
            "       -13.61855761, -13.65996823, -13.46296908, -13.46853972,\n",
            "       -13.65792882, -13.28647623, -13.17673926, -13.02765048,\n",
            "       -12.54846194, -12.8575284 , -12.6677715 , -12.54367047,\n",
            "       -12.83284818]), 'split4_test_score': array([-9.15869425, -9.55216094, -9.11174118, -9.06556194, -9.04190848,\n",
            "       -9.09571202, -9.03288777, -9.22605104, -9.30667744, -9.19693729,\n",
            "       -9.36338997, -9.24600206, -9.15396942, -8.99401526, -8.96252566,\n",
            "       -9.12649961, -9.36050003, -9.36123279, -9.37874323, -9.14276683,\n",
            "       -9.06789114, -8.82550576, -9.1987956 , -8.68293409, -8.73638332]), 'split5_test_score': array([-11.07103111, -10.79047496, -11.30171662, -10.77041242,\n",
            "       -10.43029584, -10.836716  , -11.00255949, -10.63032242,\n",
            "       -10.09039303, -10.15628643, -10.85701082, -10.56485852,\n",
            "       -10.51484756, -10.43348798, -10.42592116, -10.606045  ,\n",
            "       -10.38264082, -10.64173598, -10.36437372, -10.09664557,\n",
            "       -10.38685947, -10.61729414, -10.81807073, -10.65358641,\n",
            "       -10.60985469]), 'split6_test_score': array([-10.49642416, -10.38821758, -10.5988791 ,  -9.51181118,\n",
            "       -10.2588701 , -10.40826225, -10.08353425,  -9.35267751,\n",
            "        -9.47135836,  -9.52808512, -10.41263927,  -9.91406416,\n",
            "        -9.95516001, -10.24273983,  -9.95799194, -10.11725995,\n",
            "       -10.15216281,  -9.97963309, -10.18216695, -10.28917189,\n",
            "       -10.19859414,  -9.85283836, -10.18138094, -10.14584682,\n",
            "        -9.8947289 ]), 'split7_test_score': array([-10.73951331, -10.56499539, -11.27429746, -10.90258973,\n",
            "       -11.00910804, -10.74569249, -10.85169418, -10.97325331,\n",
            "       -10.62752145, -10.69067325, -10.06958693, -10.83718882,\n",
            "       -10.48130217, -10.26083338, -10.36169735, -10.71024903,\n",
            "       -10.56801603, -10.23724035, -10.65176465, -10.77091988,\n",
            "       -10.09390362, -10.91053647, -10.68848038, -10.23826491,\n",
            "       -10.97934336]), 'split8_test_score': array([-10.01932846, -10.65081428,  -9.96299404,  -9.69014798,\n",
            "       -10.32262553, -10.44870141, -10.44035811, -10.34475911,\n",
            "       -10.2918858 , -10.60024129, -10.38080797, -10.51592928,\n",
            "       -10.51999368, -10.56875723, -10.80616689,  -9.94537028,\n",
            "       -10.21248328,  -9.74369875, -10.35753071, -10.65860998,\n",
            "        -9.82119848,  -9.9164484 ,  -9.7962808 ,  -9.84490258,\n",
            "       -10.34513778]), 'split9_test_score': array([-10.31079864, -11.04615281, -10.56059256, -10.796411  ,\n",
            "       -10.21343951, -10.29899994, -10.27769259, -10.36758692,\n",
            "       -10.04470133,  -9.97809178, -10.30459499, -10.04737324,\n",
            "       -10.41745626, -10.46710321, -10.52307872, -10.30005006,\n",
            "        -9.83047064, -10.19268201, -10.5999282 , -10.36939455,\n",
            "       -10.54720789, -10.39745461, -10.38200898, -10.10681788,\n",
            "       -10.56621417]), 'mean_test_score': array([-10.43375517, -10.43672476, -10.338536  , -10.17239213,\n",
            "       -10.28997366, -10.36104838, -10.23340182, -10.16448577,\n",
            "        -9.97517161, -10.13884896, -10.30234283, -10.17616126,\n",
            "       -10.28976942, -10.26539482, -10.2381643 , -10.28274835,\n",
            "       -10.25507797, -10.1581798 , -10.25946436, -10.31213331,\n",
            "       -10.09923957, -10.14516326, -10.21259759, -10.04287611,\n",
            "       -10.25485821]), 'std_test_score': array([1.11149005, 1.00176979, 1.17780721, 1.06552167, 1.08962843,\n",
            "       0.95203565, 0.9848072 , 1.0573323 , 0.86357407, 1.08600639,\n",
            "       1.05172017, 1.14105364, 1.23711519, 1.26828765, 1.24080134,\n",
            "       1.17809239, 1.21370793, 1.12979912, 1.12497424, 1.06100658,\n",
            "       0.93513957, 1.10596224, 0.9819782 , 1.00605682, 1.07213292]), 'rank_test_score': array([24, 25, 22,  8, 19, 23, 11,  7,  1,  4, 20,  9, 18, 16, 12, 17, 14,\n",
            "        6, 15, 21,  3,  5, 10,  2, 13], dtype=int32), 'split0_train_score': array([-5.92909033, -5.79271433, -5.75779154, -5.78258109, -5.76389228,\n",
            "       -5.86503516, -5.70955973, -5.63365425, -5.62367474, -5.63703067,\n",
            "       -5.65518913, -5.52943738, -5.46273129, -5.4054901 , -5.46194407,\n",
            "       -5.57306641, -5.47065317, -5.45957663, -5.41861613, -5.28471357,\n",
            "       -5.53163277, -5.40064055, -5.38998868, -5.32605685, -5.19327094]), 'split1_train_score': array([-5.89370148, -5.78582203, -5.74218913, -5.81222114, -5.71664694,\n",
            "       -5.72756818, -5.64487219, -5.64802653, -5.62696785, -5.6355674 ,\n",
            "       -5.60100056, -5.57242345, -5.58386856, -5.51833253, -5.47613335,\n",
            "       -5.43896038, -5.35538873, -5.39979365, -5.40826012, -5.31022292,\n",
            "       -5.37615634, -5.39673739, -5.35698686, -5.30090493, -5.2257261 ]), 'split2_train_score': array([-5.85484584, -5.78531234, -5.81283415, -5.84383083, -5.88677971,\n",
            "       -5.77704369, -5.70009051, -5.73016631, -5.64049169, -5.6570042 ,\n",
            "       -5.6643457 , -5.69209894, -5.61568482, -5.62161799, -5.4966967 ,\n",
            "       -5.53424424, -5.40457041, -5.4219876 , -5.40198188, -5.37449946,\n",
            "       -5.4042722 , -5.36638182, -5.31644222, -5.24936188, -5.17198195]), 'split3_train_score': array([-5.65856882, -5.5580355 , -5.59989489, -5.5391483 , -5.50921812,\n",
            "       -5.63137509, -5.41585773, -5.40542532, -5.46142792, -5.34104153,\n",
            "       -5.44835311, -5.33834575, -5.31033712, -5.31771852, -5.2135347 ,\n",
            "       -5.34875434, -5.21168645, -5.17739485, -5.1238456 , -5.10883757,\n",
            "       -5.28075079, -5.17221364, -5.15156695, -5.08064512, -5.0143806 ]), 'split4_train_score': array([-5.86447925, -5.79265223, -5.77072104, -5.76066986, -5.72233722,\n",
            "       -5.83676414, -5.690154  , -5.55259625, -5.6707357 , -5.51509205,\n",
            "       -5.62717136, -5.57318859, -5.60639423, -5.61334285, -5.51452934,\n",
            "       -5.47379357, -5.42808602, -5.38603369, -5.4051065 , -5.34547802,\n",
            "       -5.39773879, -5.2879202 , -5.38196425, -5.24060371, -5.24088888]), 'split5_train_score': array([-5.88937273, -5.72279024, -5.8082775 , -5.638223  , -5.74031914,\n",
            "       -5.66175297, -5.56765691, -5.62123199, -5.54306554, -5.54848881,\n",
            "       -5.61599721, -5.59581509, -5.49154602, -5.44079626, -5.50664639,\n",
            "       -5.48353526, -5.46945481, -5.32217356, -5.30936119, -5.2940731 ,\n",
            "       -5.3655813 , -5.36275723, -5.2498255 , -5.1951979 , -5.20190183]), 'split6_train_score': array([-5.9318021 , -5.65922837, -5.78151453, -5.64249701, -5.67946957,\n",
            "       -5.77215324, -5.60609542, -5.6381951 , -5.61740947, -5.54688994,\n",
            "       -5.73535944, -5.59247773, -5.55469654, -5.50709411, -5.55990192,\n",
            "       -5.53462042, -5.45741396, -5.29487937, -5.40771572, -5.31655813,\n",
            "       -5.39650834, -5.32048048, -5.30636502, -5.24109314, -5.18663156]), 'split7_train_score': array([-5.83266822, -5.80561479, -5.70438711, -5.6648888 , -5.64038179,\n",
            "       -5.7180455 , -5.58304378, -5.56304252, -5.52901518, -5.56863806,\n",
            "       -5.68805188, -5.62963812, -5.52899009, -5.46679897, -5.46174158,\n",
            "       -5.56657977, -5.42948072, -5.30793778, -5.29258048, -5.28281602,\n",
            "       -5.42397755, -5.2280936 , -5.21658237, -5.18281276, -5.18496313]), 'split8_train_score': array([-5.85623234, -5.68616777, -5.62827812, -5.65498576, -5.65095816,\n",
            "       -5.72263511, -5.64420945, -5.6223335 , -5.51545179, -5.52050878,\n",
            "       -5.65785941, -5.54263924, -5.51688119, -5.52238559, -5.41597302,\n",
            "       -5.4802694 , -5.4027502 , -5.3327043 , -5.33462055, -5.31596072,\n",
            "       -5.38797506, -5.26761841, -5.27758488, -5.23024682, -5.17417511]), 'split9_train_score': array([-5.8192932 , -5.79937754, -5.82678591, -5.72141109, -5.68997745,\n",
            "       -5.74401273, -5.60576966, -5.52615363, -5.52744552, -5.47979217,\n",
            "       -5.65320942, -5.48484764, -5.46743394, -5.46907464, -5.38255345,\n",
            "       -5.48895914, -5.36122294, -5.43896344, -5.32999883, -5.22979807,\n",
            "       -5.48114802, -5.36303437, -5.26485819, -5.29039496, -5.24729551]), 'mean_train_score': array([-5.85300543, -5.73877151, -5.74326739, -5.70604569, -5.69999804,\n",
            "       -5.74563858, -5.61673094, -5.59408254, -5.57556854, -5.54500536,\n",
            "       -5.63465372, -5.55509119, -5.51385638, -5.48826516, -5.44896545,\n",
            "       -5.49227829, -5.39907074, -5.35414449, -5.3432087 , -5.28629576,\n",
            "       -5.40457412, -5.31658777, -5.29121649, -5.23373181, -5.18412156]), 'std_train_score': array([0.07380904, 0.07768465, 0.07332103, 0.08951516, 0.09178763,\n",
            "       0.06779482, 0.08162741, 0.08328642, 0.06493085, 0.08782765,\n",
            "       0.07171028, 0.08985454, 0.08510391, 0.0865277 , 0.09176456,\n",
            "       0.0629418 , 0.07331151, 0.08023922, 0.08569833, 0.06957676,\n",
            "       0.06358583, 0.07222692, 0.07130827, 0.06642127, 0.06197545])}\n",
            "{'bagging_fraction': 0.7, 'feature_fraction': 0.8}\n",
            "-9.975171609924274\n",
            "rmse: 3.1583495072465104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l2xXiw3DS4gD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 正则化参数"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:27:52.642169Z",
          "start_time": "2019-04-12T18:21:45.309216Z"
        },
        "id": "ZOS6nTDcS4gE",
        "colab_type": "code",
        "outputId": "452ff626-1a7f-446d-a8b1-d99f0edd50da",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params_test6 = {\n",
        "    'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
        "    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]\n",
        "}\n",
        "model_lgb = lgb.LGBMRegressor(objective='regression',\n",
        "                              num_leaves=16,\n",
        "                              learning_rate=0.1,\n",
        "                              n_estimators=43,\n",
        "                              max_depth=6,\n",
        "                              metric='rmse',\n",
        "                              min_child_samples=20,\n",
        "                              min_child_weight=0.001,\n",
        "                              feature_fraction=0.8,\n",
        "                              bagging_fraction=0.7)\n",
        "gsearch6 = GridSearchCV(estimator=model_lgb, param_grid=params_test6,\n",
        "                        scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
        "gsearch6.fit(xtrain, ytrain)\n",
        "print_(gsearch6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 49 candidates, totalling 490 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   30.3s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=4)]: Done 490 out of 490 | elapsed:  6.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'mean_fit_time': array([2.54953973, 2.48352652, 2.44470265, 2.40663574, 2.44401784,\n",
            "       3.07249022, 3.06396115, 2.58210669, 2.4684319 , 2.49551215,\n",
            "       2.5604156 , 2.50873158, 2.49764013, 2.68728771, 2.49779704,\n",
            "       2.57869885, 2.59264336, 2.55016713, 2.55847514, 2.60326304,\n",
            "       2.5180795 , 2.59333334, 2.56451044, 2.54676473, 2.88733516,\n",
            "       2.73384731, 2.60004342, 2.62316666, 2.47140055, 2.5543318 ,\n",
            "       2.62194357, 2.62157247, 2.66752541, 2.6159936 , 2.75703018,\n",
            "       2.63211706, 2.64107139, 2.85074286, 2.56648724, 2.63695562,\n",
            "       2.6791122 , 2.55771363, 2.59988146, 2.67818027, 2.7400804 ,\n",
            "       2.78853831, 2.69739177, 2.81976368, 2.52642329]), 'std_fit_time': array([0.15366366, 0.14704099, 0.11413499, 0.07396026, 0.11799336,\n",
            "       0.58883204, 0.49783955, 0.21547983, 0.12424784, 0.13143344,\n",
            "       0.18417971, 0.21139881, 0.16976621, 0.19555309, 0.1566916 ,\n",
            "       0.19838069, 0.09192913, 0.17775724, 0.13742148, 0.16377159,\n",
            "       0.18933214, 0.20228498, 0.33839424, 0.16144058, 0.18759818,\n",
            "       0.21764128, 0.18794214, 0.11011909, 0.22626988, 0.20170835,\n",
            "       0.15033391, 0.13951361, 0.15332539, 0.20235692, 0.19255141,\n",
            "       0.16860921, 0.31005595, 0.3055608 , 0.22308064, 0.19377802,\n",
            "       0.20454006, 0.16745538, 0.29593496, 0.15902527, 0.21608193,\n",
            "       0.08518568, 0.25511256, 0.15009249, 0.25339878]), 'mean_score_time': array([0.11405017, 0.11446238, 0.14070315, 0.12160242, 0.11857255,\n",
            "       0.20740681, 0.14463158, 0.12676663, 0.13939874, 0.12030201,\n",
            "       0.13375635, 0.12471721, 0.12180612, 0.11669064, 0.10942442,\n",
            "       0.13942366, 0.11664643, 0.11364696, 0.13259084, 0.12265708,\n",
            "       0.12909982, 0.13163605, 0.12569666, 0.11522896, 0.14298561,\n",
            "       0.13314724, 0.12328365, 0.12853229, 0.10379074, 0.13516936,\n",
            "       0.12540016, 0.12674742, 0.12799518, 0.13832724, 0.1449703 ,\n",
            "       0.13331199, 0.11036081, 0.13045316, 0.12465968, 0.11825013,\n",
            "       0.15049858, 0.13324575, 0.12492974, 0.12528334, 0.15150301,\n",
            "       0.11736367, 0.12595119, 0.12347534, 0.11049945]), 'std_score_time': array([0.022299  , 0.02597415, 0.0390019 , 0.0311727 , 0.03360687,\n",
            "       0.10067077, 0.03775711, 0.03274948, 0.03205792, 0.02820125,\n",
            "       0.0311854 , 0.03449726, 0.03500621, 0.02656101, 0.01801946,\n",
            "       0.0407223 , 0.02226015, 0.0279177 , 0.04087169, 0.03264091,\n",
            "       0.03032587, 0.04035025, 0.02621139, 0.03333214, 0.04577393,\n",
            "       0.03720757, 0.03189518, 0.03322997, 0.01335428, 0.02715121,\n",
            "       0.02619645, 0.02083023, 0.0429629 , 0.03493412, 0.03536763,\n",
            "       0.03118881, 0.01662911, 0.03535014, 0.03557299, 0.02430966,\n",
            "       0.0429835 , 0.02993915, 0.03242775, 0.02834889, 0.03831083,\n",
            "       0.02360315, 0.01935984, 0.01844476, 0.0360794 ]), 'param_reg_alpha': masked_array(data=[0, 0, 0, 0, 0, 0, 0, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
            "                   0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
            "                   0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.08, 0.08,\n",
            "                   0.08, 0.08, 0.08, 0.08, 0.08, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
            "                   0.3, 0.3, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_reg_lambda': masked_array(data=[0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5, 0, 0.001, 0.01,\n",
            "                   0.03, 0.08, 0.3, 0.5, 0, 0.001, 0.01, 0.03, 0.08, 0.3,\n",
            "                   0.5, 0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5, 0, 0.001,\n",
            "                   0.01, 0.03, 0.08, 0.3, 0.5, 0, 0.001, 0.01, 0.03, 0.08,\n",
            "                   0.3, 0.5, 0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False, False, False, False, False,\n",
            "                   False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'reg_alpha': 0, 'reg_lambda': 0}, {'reg_alpha': 0, 'reg_lambda': 0.001}, {'reg_alpha': 0, 'reg_lambda': 0.01}, {'reg_alpha': 0, 'reg_lambda': 0.03}, {'reg_alpha': 0, 'reg_lambda': 0.08}, {'reg_alpha': 0, 'reg_lambda': 0.3}, {'reg_alpha': 0, 'reg_lambda': 0.5}, {'reg_alpha': 0.001, 'reg_lambda': 0}, {'reg_alpha': 0.001, 'reg_lambda': 0.001}, {'reg_alpha': 0.001, 'reg_lambda': 0.01}, {'reg_alpha': 0.001, 'reg_lambda': 0.03}, {'reg_alpha': 0.001, 'reg_lambda': 0.08}, {'reg_alpha': 0.001, 'reg_lambda': 0.3}, {'reg_alpha': 0.001, 'reg_lambda': 0.5}, {'reg_alpha': 0.01, 'reg_lambda': 0}, {'reg_alpha': 0.01, 'reg_lambda': 0.001}, {'reg_alpha': 0.01, 'reg_lambda': 0.01}, {'reg_alpha': 0.01, 'reg_lambda': 0.03}, {'reg_alpha': 0.01, 'reg_lambda': 0.08}, {'reg_alpha': 0.01, 'reg_lambda': 0.3}, {'reg_alpha': 0.01, 'reg_lambda': 0.5}, {'reg_alpha': 0.03, 'reg_lambda': 0}, {'reg_alpha': 0.03, 'reg_lambda': 0.001}, {'reg_alpha': 0.03, 'reg_lambda': 0.01}, {'reg_alpha': 0.03, 'reg_lambda': 0.03}, {'reg_alpha': 0.03, 'reg_lambda': 0.08}, {'reg_alpha': 0.03, 'reg_lambda': 0.3}, {'reg_alpha': 0.03, 'reg_lambda': 0.5}, {'reg_alpha': 0.08, 'reg_lambda': 0}, {'reg_alpha': 0.08, 'reg_lambda': 0.001}, {'reg_alpha': 0.08, 'reg_lambda': 0.01}, {'reg_alpha': 0.08, 'reg_lambda': 0.03}, {'reg_alpha': 0.08, 'reg_lambda': 0.08}, {'reg_alpha': 0.08, 'reg_lambda': 0.3}, {'reg_alpha': 0.08, 'reg_lambda': 0.5}, {'reg_alpha': 0.3, 'reg_lambda': 0}, {'reg_alpha': 0.3, 'reg_lambda': 0.001}, {'reg_alpha': 0.3, 'reg_lambda': 0.01}, {'reg_alpha': 0.3, 'reg_lambda': 0.03}, {'reg_alpha': 0.3, 'reg_lambda': 0.08}, {'reg_alpha': 0.3, 'reg_lambda': 0.3}, {'reg_alpha': 0.3, 'reg_lambda': 0.5}, {'reg_alpha': 0.5, 'reg_lambda': 0}, {'reg_alpha': 0.5, 'reg_lambda': 0.001}, {'reg_alpha': 0.5, 'reg_lambda': 0.01}, {'reg_alpha': 0.5, 'reg_lambda': 0.03}, {'reg_alpha': 0.5, 'reg_lambda': 0.08}, {'reg_alpha': 0.5, 'reg_lambda': 0.3}, {'reg_alpha': 0.5, 'reg_lambda': 0.5}], 'split0_test_score': array([-9.01199   , -9.00131135, -9.00143522, -9.00171059, -8.99707388,\n",
            "       -9.19147259, -9.37593679, -9.01199641, -9.00131786, -9.00144173,\n",
            "       -9.00171709, -8.99708067, -9.19147339, -9.37593522, -9.00136266,\n",
            "       -9.00137643, -9.00150032, -9.00177573, -8.99714185, -9.1914806 ,\n",
            "       -9.3759211 , -9.00149286, -9.00150664, -9.00163057, -9.00190608,\n",
            "       -9.24750365, -9.19149667, -9.37588977, -9.00181876, -9.00183254,\n",
            "       -9.00195659, -9.00223234, -9.18795981, -9.07476895, -9.41419063,\n",
            "       -9.29544463, -9.29544841, -9.29548244, -9.10074163, -9.30137313,\n",
            "       -9.18418313, -9.2379781 , -9.28182507, -9.28182787, -9.28185316,\n",
            "       -9.27289768, -9.17452332, -9.11187467, -9.35357522]), 'split1_test_score': array([-9.39830226, -9.39831672, -9.39576551, -9.39267185, -9.58590391,\n",
            "       -9.45019714, -9.39664311, -9.39830922, -9.39832368, -9.19748802,\n",
            "       -9.39267795, -9.58590736, -9.45020443, -9.39665033, -9.39837192,\n",
            "       -9.39838638, -9.19754096, -9.39951482, -9.58593838, -9.54378475,\n",
            "       -9.45257329, -9.19752962, -9.19754252, -9.39935528, -9.39963625,\n",
            "       -9.51899136, -9.54380648, -9.45274281, -9.58543627, -9.58544555,\n",
            "       -9.58552908, -9.39865479, -9.61583572, -9.4378499 , -9.47513008,\n",
            "       -9.51355358, -9.51356546, -9.63163391, -9.876787  , -9.91805354,\n",
            "       -9.37950531, -9.56274623, -9.79231862, -9.79231899, -9.79232238,\n",
            "       -9.6392721 , -9.48768097, -9.54636939, -9.18114156]), 'split2_test_score': array([-9.79997774, -9.79997487, -9.79994918, -9.79989258, -9.67083122,\n",
            "       -9.602518  , -9.60101198, -9.79997905, -9.7999762 , -9.79995051,\n",
            "       -9.79989391, -9.6708354 , -9.60252136, -9.60101451, -9.79999089,\n",
            "       -9.79998803, -9.79996239, -9.79990589, -9.670873  , -9.60255156,\n",
            "       -9.60103731, -9.80001727, -9.80001442, -9.79998887, -9.79993258,\n",
            "       -9.52765895, -9.60261875, -9.60457074, -9.90117454, -9.9011679 ,\n",
            "       -9.90110824, -9.81093394, -9.56613251, -9.70965308, -9.60469963,\n",
            "       -9.90510457, -9.90510332, -9.90509208, -9.86533678, -9.86530469,\n",
            "       -9.59884931, -9.8497071 , -9.92711346, -9.92711363, -9.92711522,\n",
            "       -9.97115155, -9.53808822, -9.52201467, -9.52345537]), 'split3_test_score': array([-12.54367047, -12.54368199, -12.58713471, -12.62689088,\n",
            "       -12.59894431, -12.97591641, -12.72249566, -12.54367225,\n",
            "       -12.54368376, -12.58713702, -12.62689187, -12.59894319,\n",
            "       -12.97591587, -12.72249823, -12.58705907, -12.58706894,\n",
            "       -12.62668375, -12.56784794, -12.59893308, -12.97591099,\n",
            "       -12.72252138, -12.62659496, -12.62660582, -12.62670357,\n",
            "       -12.59852421, -12.76717595, -12.6852168 , -12.72257287,\n",
            "       -12.59823542, -12.59824316, -12.59831288, -12.766646  ,\n",
            "       -12.75173774, -12.70361738, -12.72270186, -12.7713228 ,\n",
            "       -12.77132968, -12.77139162, -12.87302622, -13.08154564,\n",
            "       -12.77695705, -12.56803215, -13.06893243, -13.06893137,\n",
            "       -13.06892192, -13.07257001, -12.87282881, -12.76731413,\n",
            "       -12.67283952]), 'split4_test_score': array([-8.68293409, -8.68294018, -8.52077842, -8.52095391, -8.93473531,\n",
            "       -8.88178797, -8.77209051, -8.68293889, -8.68294499, -8.52078484,\n",
            "       -8.52096032, -8.93473778, -8.88178866, -8.77209091, -8.52075488,\n",
            "       -8.52076365, -8.52084258, -8.5210181 , -8.93475996, -8.88179485,\n",
            "       -8.77209457, -8.52088321, -8.52089199, -8.52097096, -8.81709054,\n",
            "       -8.93480933, -8.88180864, -9.02294924, -8.69930659, -8.69931426,\n",
            "       -8.81720196, -8.79405064, -8.90330213, -8.86144452, -8.85763613,\n",
            "       -8.80066103, -8.8006642 , -8.87266519, -8.86605678, -9.00975627,\n",
            "       -8.71686258, -8.90651492, -8.74551399, -8.74552096, -8.81656129,\n",
            "       -8.81669653, -8.91821722, -8.88952302, -9.00781759]), 'split5_test_score': array([-10.65358641, -10.65356873, -10.65340968, -10.65305667,\n",
            "       -10.82128368, -10.69714853, -10.5599499 , -10.65358016,\n",
            "       -10.65356249, -10.65340344, -10.65305044, -10.8212759 ,\n",
            "       -10.69713963, -10.55994137, -10.65352391, -10.65350623,\n",
            "       -10.65334722, -10.76497453, -10.82120589, -10.69705959,\n",
            "       -10.55986455, -10.65339894, -10.65338127, -10.65322235,\n",
            "       -10.79345761, -10.82105034, -10.69688176, -10.55969391,\n",
            "       -10.67760772, -10.67758896, -10.67742022, -10.79315358,\n",
            "       -10.79215019, -10.72540967, -10.49746986, -10.8253584 ,\n",
            "       -10.82534302, -10.66677796, -10.66643849, -10.56979976,\n",
            "       -10.66897784, -10.48604375, -10.57406578, -10.57405062,\n",
            "       -10.57391423, -10.57361159, -10.76230328, -10.4253366 ,\n",
            "       -10.51321055]), 'split6_test_score': array([-10.14584682, -10.14582363, -10.14561505, -10.14515237,\n",
            "       -10.06205502,  -9.95072887,  -9.84649732, -10.14583857,\n",
            "       -10.14581539, -10.14560682, -10.14514415, -10.06204842,\n",
            "        -9.9507239 ,  -9.84649565, -10.00122059, -10.14574121,\n",
            "       -10.14553271, -10.14507016, -10.061989  ,  -9.95067921,\n",
            "        -9.84648063,  -9.88629518,  -9.88628011, -10.14536808,\n",
            "       -10.14490581, -10.06185706,  -9.95057994,  -9.91180297,\n",
            "        -9.63750246,  -9.63749375,  -9.63741543,  -9.92954562,\n",
            "       -10.02299383,  -9.94113919,  -9.91165072,  -9.91862458,\n",
            "        -9.91860727,  -9.97334276,  -9.99104683,  -9.93258173,\n",
            "        -9.92441888,  -9.88226609,  -9.83606012,  -9.83605177,\n",
            "        -9.83597666,  -9.90588179,  -9.79146961, -10.21303117,\n",
            "        -9.99276373]), 'split7_test_score': array([-10.23826491, -10.23827518, -10.23836771, -10.43625709,\n",
            "       -10.44922314, -10.44766821, -10.46140574, -10.23826882,\n",
            "       -10.2382791 , -10.23837163, -10.43626023, -10.44922647,\n",
            "       -10.44766979, -10.46141408, -10.23830408, -10.23831436,\n",
            "       -10.23840693, -10.43628852, -10.49283811, -10.44768397,\n",
            "       -10.46148912, -10.23838249, -10.23839278, -10.45457896,\n",
            "       -10.43635146, -10.60371885, -10.32281718, -10.65578191,\n",
            "       -10.39048178, -10.3904868 , -10.39053212, -10.60794606,\n",
            "       -10.43425949, -10.63716615, -10.55119829, -10.43458968,\n",
            "       -10.43459513, -10.43464421, -10.43700499, -10.43722138,\n",
            "       -10.6428374 , -10.4992263 , -10.57410008, -10.57409911,\n",
            "       -10.57409048, -10.57407182, -10.27657825, -10.39168361,\n",
            "       -10.38119576]), 'split8_test_score': array([ -9.84490258,  -9.84490851,  -9.84496187,  -9.84508062,\n",
            "        -9.82279937,  -9.76270801,  -9.76333213,  -9.84490228,\n",
            "        -9.8449082 ,  -9.84496157,  -9.84508033,  -9.82280058,\n",
            "        -9.76271222,  -9.76333827,  -9.84489954,  -9.84490547,\n",
            "        -9.84495886,  -9.84507768,  -9.8228114 ,  -9.76275011,\n",
            "       -10.02250316,  -9.80441957,  -9.8044257 ,  -9.80448085,\n",
            "        -9.95215349,  -9.95229726, -10.13515659, -10.02247084,\n",
            "        -9.80455165,  -9.80455778,  -9.9521167 ,  -9.95218115,\n",
            "        -9.95232747, -10.00147009,  -9.95663617,  -9.94261355,\n",
            "        -9.94261588,  -9.9426369 ,  -9.94268395, -10.00248717,\n",
            "        -9.99923532,  -9.82182838,  -9.96788374,  -9.96788503,\n",
            "        -9.96789674,  -9.92300267, -10.16153221, -10.17403595,\n",
            "        -9.65657139]), 'split9_test_score': array([-10.10681788, -10.10681383, -10.1067774 , -10.10669693,\n",
            "       -10.09813336, -10.62321553, -10.33956059, -10.10681602,\n",
            "       -10.10681196, -10.10677555, -10.10669508, -10.09813266,\n",
            "       -10.56478909, -10.33955318, -10.10679924, -10.10679519,\n",
            "       -10.10675881, -10.07450051, -10.11675903, -10.56471587,\n",
            "       -10.60020001, -10.10676203, -10.10675798, -10.10672169,\n",
            "       -10.30148566, -10.11678129, -10.56455348, -10.65113678,\n",
            "       -10.06162845, -10.06162766, -10.27475028, -10.14676706,\n",
            "       -10.11683724, -10.49496786, -10.42855412, -10.11008454,\n",
            "       -10.11009059, -10.11014494, -10.11026595, -10.68138079,\n",
            "       -10.36367213, -10.48958598, -10.52986451, -10.52985174,\n",
            "       -10.52973689, -10.64980353, -10.64788053, -10.37722696,\n",
            "       -10.28796134]), 'mean_test_score': array([-10.04287611, -10.04180559, -10.02970146, -10.05309431,\n",
            "       -10.10428291, -10.15858433, -10.08421408, -10.04287697,\n",
            "       -10.04180646, -10.00982377, -10.0530951 , -10.10428343,\n",
            "       -10.15275193, -10.08421489, -10.01553573, -10.02996719,\n",
            "       -10.01379519, -10.05582863, -10.11049908, -10.16212303,\n",
            "       -10.14171647,  -9.98387005,  -9.98387237, -10.05156543,\n",
            "       -10.12467204, -10.15537078, -10.15765994, -10.19811513,\n",
            "       -10.0361735 , -10.03617499, -10.08395261, -10.12042162,\n",
            "       -10.13460076, -10.15887173, -10.14226119, -10.15210735,\n",
            "       -10.15210791, -10.16078817, -10.17337208, -10.28039308,\n",
            "       -10.12573487, -10.13068769, -10.23025472, -10.23025205,\n",
            "       -10.23731388, -10.24031738, -10.1632882 , -10.14200185,\n",
            "       -10.05726589]), 'std_test_score': array([1.00605682, 1.00715826, 1.04072703, 1.05616819, 1.00021781,\n",
            "       1.10108151, 1.02470397, 1.00605508, 1.00715651, 1.05444719,\n",
            "       1.05616613, 1.00021568, 1.09875569, 1.02470366, 1.03986378,\n",
            "       1.04056541, 1.06413839, 1.04825792, 1.00177722, 1.09309095,\n",
            "       1.02444956, 1.06436082, 1.06435926, 1.0572741 , 1.01704077,\n",
            "       1.03188535, 1.00887871, 1.00064119, 1.02022079, 1.02021921,\n",
            "       1.00471124, 1.0678379 , 1.02174859, 1.03961673, 1.00262634,\n",
            "       1.02658962, 1.0265894 , 1.00007629, 1.03691645, 1.05877213,\n",
            "       1.06902972, 0.95918484, 1.09612823, 1.09612616, 1.0866761 ,\n",
            "       1.09727099, 1.0714899 , 1.02393245, 1.00197697]), 'rank_test_score': array([12, 10,  6, 15, 22, 38, 20, 13, 11,  3, 16, 23, 35, 21,  5,  7,  4,\n",
            "       17, 24, 41, 30,  1,  2, 14, 26, 36, 37, 44,  8,  9, 19, 25, 29, 39,\n",
            "       32, 33, 34, 40, 43, 49, 27, 28, 46, 45, 47, 48, 42, 31, 18],\n",
            "      dtype=int32), 'split0_train_score': array([-5.32605685, -5.32601203, -5.3265877 , -5.32786632, -5.33258243,\n",
            "       -5.30813058, -5.35832611, -5.32609048, -5.32604556, -5.32662123,\n",
            "       -5.32789985, -5.33261591, -5.30816429, -5.35835957, -5.32628342,\n",
            "       -5.32634739, -5.32692304, -5.32820162, -5.33291728, -5.30846772,\n",
            "       -5.35866071, -5.32695425, -5.32701822, -5.32759382, -5.32887231,\n",
            "       -5.37184168, -5.30914208, -5.35932998, -5.3286319 , -5.32869585,\n",
            "       -5.32927136, -5.33054961, -5.29032815, -5.25749457, -5.28718094,\n",
            "       -5.27737059, -5.27743254, -5.27798996, -5.334497  , -5.30728932,\n",
            "       -5.312527  , -5.30849211, -5.32347781, -5.32353965, -5.32409612,\n",
            "       -5.31230185, -5.28619217, -5.359714  , -5.33126245]), 'split1_train_score': array([-5.30090493, -5.30096555, -5.29819581, -5.32760599, -5.30163774,\n",
            "       -5.28361601, -5.31280931, -5.30093772, -5.30099833, -5.31311442,\n",
            "       -5.32764005, -5.30167155, -5.28364937, -5.31284227, -5.30123278,\n",
            "       -5.30129339, -5.31341117, -5.33013675, -5.30197584, -5.30845219,\n",
            "       -5.33950276, -5.31345842, -5.31351966, -5.32951956, -5.33081931,\n",
            "       -5.32766271, -5.30911978, -5.34014988, -5.29922309, -5.29928718,\n",
            "       -5.29986389, -5.32430955, -5.23285392, -5.27203872, -5.36540779,\n",
            "       -5.29241525, -5.29247807, -5.25224687, -5.27328742, -5.29058268,\n",
            "       -5.3655033 , -5.27640866, -5.27091576, -5.27097898, -5.27154788,\n",
            "       -5.28014143, -5.25127354, -5.35684642, -5.37381794]), 'split2_train_score': array([-5.24936188, -5.24942347, -5.24997775, -5.25120886, -5.23829167,\n",
            "       -5.32676401, -5.26902808, -5.24939477, -5.24945636, -5.25001064,\n",
            "       -5.25124174, -5.2383247 , -5.32679659, -5.26906043, -5.2496908 ,\n",
            "       -5.2497524 , -5.25030665, -5.25153772, -5.23862203, -5.32708984,\n",
            "       -5.26935162, -5.25034876, -5.25041034, -5.25096455, -5.25219553,\n",
            "       -5.2720731 , -5.32774159, -5.25636361, -5.23650772, -5.23657042,\n",
            "       -5.23713456, -5.22485762, -5.27355615, -5.3142551 , -5.25797711,\n",
            "       -5.28312947, -5.28319181, -5.28375275, -5.25644064, -5.25956378,\n",
            "       -5.35164176, -5.39687392, -5.27270019, -5.27276745, -5.27337268,\n",
            "       -5.26111453, -5.32500251, -5.31759338, -5.38219615]), 'split3_train_score': array([-5.08064512, -5.08070388, -5.06565329, -5.12035582, -5.12938652,\n",
            "       -5.10129418, -5.11454639, -5.08067682, -5.08073558, -5.0656843 ,\n",
            "       -5.1203866 , -5.12941717, -5.10132494, -5.1145772 , -5.06538461,\n",
            "       -5.0654425 , -5.11953757, -5.09424929, -5.12969297, -5.10160182,\n",
            "       -5.11485454, -5.11959012, -5.11964645, -5.12015337, -5.12749857,\n",
            "       -5.13061557, -5.0640733 , -5.11547092, -5.12734579, -5.12740203,\n",
            "       -5.12790804, -5.12934576, -5.06730789, -5.06905374, -5.11701241,\n",
            "       -5.10032872, -5.10038589, -5.10090039, -5.0878322 , -5.07303273,\n",
            "       -5.11652118, -5.12616086, -5.21804427, -5.21809593, -5.21856084,\n",
            "       -5.17895331, -5.17023643, -5.17049399, -5.11862923]), 'split4_train_score': array([-5.24060371, -5.24066554, -5.2053294 , -5.20659401, -5.27810343,\n",
            "       -5.28370564, -5.26001026, -5.24063683, -5.24069866, -5.20536276,\n",
            "       -5.20662737, -5.27813651, -5.28373827, -5.26004325, -5.20503043,\n",
            "       -5.20509369, -5.205663  , -5.20692758, -5.27843425, -5.28403198,\n",
            "       -5.26034021, -5.20569776, -5.20576102, -5.2063303 , -5.26470624,\n",
            "       -5.279096  , -5.28468477, -5.28962639, -5.23956973, -5.23963044,\n",
            "       -5.2651546 , -5.24835735, -5.25673541, -5.215316  , -5.29647444,\n",
            "       -5.20949491, -5.20955761, -5.2509795 , -5.25815965, -5.27044951,\n",
            "       -5.25702045, -5.27675971, -5.33738101, -5.33743901, -5.29909314,\n",
            "       -5.3002552 , -5.24003148, -5.20816476, -5.27112098]), 'split5_train_score': array([-5.1951979 , -5.19525775, -5.19579628, -5.19699249, -5.17553107,\n",
            "       -5.26598699, -5.23565409, -5.19523044, -5.19529028, -5.19582881,\n",
            "       -5.19702502, -5.17556269, -5.26601766, -5.23568481, -5.19552325,\n",
            "       -5.1955831 , -5.19612161, -5.19540554, -5.17584728, -5.26629373,\n",
            "       -5.23596135, -5.19617406, -5.1962339 , -5.19677237, -5.2079279 ,\n",
            "       -5.1764798 , -5.26690731, -5.23657597, -5.19214179, -5.19220214,\n",
            "       -5.19274522, -5.20959722, -5.21276052, -5.247694  , -5.17753706,\n",
            "       -5.25085385, -5.250914  , -5.20453849, -5.20573509, -5.24821518,\n",
            "       -5.24948053, -5.19950537, -5.22035299, -5.22041263, -5.22094935,\n",
            "       -5.22214156, -5.24208091, -5.21590672, -5.20254402]), 'split6_train_score': array([-5.24109314, -5.24115233, -5.24168499, -5.24286819, -5.23523809,\n",
            "       -5.26020243, -5.29673757, -5.24112495, -5.24118414, -5.2417168 ,\n",
            "       -5.2429    , -5.23527077, -5.26023535, -5.29676912, -5.32414534,\n",
            "       -5.24147046, -5.24200311, -5.24318628, -5.23556485, -5.26053155,\n",
            "       -5.29705312, -5.23892463, -5.23898473, -5.24263944, -5.24382257,\n",
            "       -5.23621847, -5.26118987, -5.28979094, -5.24602022, -5.24608025,\n",
            "       -5.24662046, -5.24756623, -5.26622836, -5.27038571, -5.29137212,\n",
            "       -5.25178305, -5.25184713, -5.2238474 , -5.22392959, -5.16021383,\n",
            "       -5.27734059, -5.21949623, -5.18887319, -5.18893589, -5.18950006,\n",
            "       -5.18388137, -5.25396126, -5.26788233, -5.30397882]), 'split7_train_score': array([-5.18281276, -5.18287456, -5.18343062, -5.21274412, -5.21691224,\n",
            "       -5.21306648, -5.20046005, -5.18284535, -5.18290714, -5.1834632 ,\n",
            "       -5.2127765 , -5.21694506, -5.2130983 , -5.20049197, -5.18313862,\n",
            "       -5.18320041, -5.18375646, -5.21306794, -5.22483793, -5.21338473,\n",
            "       -5.20077931, -5.18379044, -5.18385223, -5.21233415, -5.21371567,\n",
            "       -5.26790426, -5.23262553, -5.24046199, -5.2007463 , -5.20080962,\n",
            "       -5.20137943, -5.26620072, -5.24083054, -5.18894087, -5.23789068,\n",
            "       -5.24276952, -5.24283299, -5.24340419, -5.24468943, -5.248988  ,\n",
            "       -5.2304104 , -5.21355401, -5.22826333, -5.22832795, -5.22890939,\n",
            "       -5.23020088, -5.27974225, -5.26349946, -5.23350075]), 'split8_train_score': array([-5.23024682, -5.23030939, -5.23087241, -5.232123  , -5.22126172,\n",
            "       -5.26131773, -5.28646152, -5.23027962, -5.23034219, -5.23090521,\n",
            "       -5.23215579, -5.22129468, -5.2613496 , -5.28649285, -5.23057487,\n",
            "       -5.23063743, -5.23120044, -5.23245098, -5.22159128, -5.26163643,\n",
            "       -5.34769149, -5.29658035, -5.29664159, -5.29719262, -5.25341623,\n",
            "       -5.25644323, -5.25271614, -5.34831089, -5.29820453, -5.29826574,\n",
            "       -5.25381898, -5.25502923, -5.25805478, -5.30867364, -5.2451193 ,\n",
            "       -5.26608402, -5.26614507, -5.26669442, -5.26791464, -5.30096348,\n",
            "       -5.31048656, -5.28543359, -5.30218183, -5.30224306, -5.30279403,\n",
            "       -5.26212033, -5.25223215, -5.30308777, -5.27433069]), 'split9_train_score': array([-5.29039496, -5.29045637, -5.2910089 , -5.29223616, -5.27256527,\n",
            "       -5.32595708, -5.27786834, -5.2904269 , -5.29048831, -5.29104084,\n",
            "       -5.2922681 , -5.27259869, -5.32043418, -5.27790092, -5.29071438,\n",
            "       -5.29077578, -5.2913283 , -5.307747  , -5.23665016, -5.32072187,\n",
            "       -5.25710425, -5.29135331, -5.29141471, -5.2919672 , -5.26672995,\n",
            "       -5.23731773, -5.32136128, -5.27835781, -5.25600828, -5.25606956,\n",
            "       -5.2528271 , -5.23959585, -5.23898726, -5.30065753, -5.27722849,\n",
            "       -5.31891855, -5.31898285, -5.31956141, -5.32084648, -5.27933861,\n",
            "       -5.28448144, -5.2772145 , -5.2525987 , -5.25265857, -5.25319723,\n",
            "       -5.28219527, -5.3074904 , -5.23632885, -5.27451358]), 'mean_train_score': array([-5.23373181, -5.23378209, -5.22885371, -5.2410595 , -5.24015102,\n",
            "       -5.26300411, -5.26119017, -5.23376439, -5.23381466, -5.23037482,\n",
            "       -5.2410921 , -5.24018377, -5.26248085, -5.26122224, -5.23717185,\n",
            "       -5.22895966, -5.23602513, -5.24029107, -5.23761339, -5.26522119,\n",
            "       -5.26812993, -5.24228721, -5.24234829, -5.24754674, -5.24897043,\n",
            "       -5.25556526, -5.26295616, -5.27544384, -5.24243994, -5.24250132,\n",
            "       -5.24067236, -5.24754091, -5.2337643 , -5.24445099, -5.25532003,\n",
            "       -5.24931479, -5.2493768 , -5.24239154, -5.24733322, -5.24386371,\n",
            "       -5.27554132, -5.2579899 , -5.26147891, -5.26153991, -5.25820207,\n",
            "       -5.25133057, -5.26082431, -5.26995177, -5.27658946]), 'std_train_score': array([0.06642127, 0.06640708, 0.07013009, 0.06027588, 0.05673112,\n",
            "       0.06291396, 0.06339386, 0.0664216 , 0.06640739, 0.0717264 ,\n",
            "       0.06027651, 0.05673195, 0.06237849, 0.06339446, 0.07607512,\n",
            "       0.07045906, 0.06032243, 0.06763452, 0.05547939, 0.06363599,\n",
            "       0.07050142, 0.06293412, 0.0629354 , 0.06272684, 0.05602324,\n",
            "       0.06501668, 0.07276166, 0.0675564 , 0.0561412 , 0.05614283,\n",
            "       0.05373826, 0.05401506, 0.05931392, 0.06961688, 0.06496302,\n",
            "       0.05722576, 0.05722728, 0.05611792, 0.06482653, 0.06921224,\n",
            "       0.06700978, 0.06921646, 0.04637707, 0.0463774 , 0.04119839,\n",
            "       0.04380899, 0.04062013, 0.06064851, 0.07528318])}\n",
            "{'reg_alpha': 0.03, 'reg_lambda': 0}\n",
            "-9.983870049968814\n",
            "rmse: 3.159726261872825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f0LZkwvUS4gI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 降低learning_rate"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-12T18:31:27.302467Z",
          "start_time": "2019-04-12T18:29:36.384033Z"
        },
        "id": "_89hOJfUS4gI",
        "colab_type": "code",
        "outputId": "281c5cdc-d0d5-4533-f255-dcf58c8933e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# params = {\n",
        "#     'boosting_type': 'gbdt',\n",
        "#     'objective': 'regression',\n",
        "\n",
        "#     'learning_rate': 0.005,\n",
        "#     'num_leaves': 80,\n",
        "#     'max_depth': 7,\n",
        "#     'min_data_in_leaf': 20,\n",
        "\n",
        "#     'subsample': 1,\n",
        "#     'colsample_bytree': 0.7,\n",
        "# }\n",
        "\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'num_leaves': 16,\n",
        "    'learning_rate': 0.005,\n",
        "#     'n_estimators': 43,\n",
        "    'max_depth': 6,\n",
        "    'metric': 'rmse',\n",
        "    'min_child_samples': 20,\n",
        "    'min_child_weight': 0.001,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.7\n",
        "}\n",
        "\n",
        "data_train = lgb.Dataset(xtrain, ytrain, silent=True)\n",
        "cv_results = lgb.cv(\n",
        "    params, data_train, num_boost_round=10000, nfold=10, stratified=False, shuffle=True, metrics='rmse',\n",
        "    early_stopping_rounds=50, verbose_eval=100, show_stdv=True)\n",
        "\n",
        "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
        "print('best cv score:', cv_results['rmse-mean'][-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100]\tcv_agg's rmse: 4.12491 + 0.260789\n",
            "[200]\tcv_agg's rmse: 3.64173 + 0.256248\n",
            "[300]\tcv_agg's rmse: 3.40297 + 0.244103\n",
            "[400]\tcv_agg's rmse: 3.28311 + 0.227424\n",
            "[500]\tcv_agg's rmse: 3.21311 + 0.211949\n",
            "[600]\tcv_agg's rmse: 3.17422 + 0.202855\n",
            "[700]\tcv_agg's rmse: 3.1572 + 0.198252\n",
            "[800]\tcv_agg's rmse: 3.14919 + 0.194758\n",
            "[900]\tcv_agg's rmse: 3.14666 + 0.190449\n",
            "best n_estimators: 941\n",
            "best cv score: 3.145967142110073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lZfy9W_BcoHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Cross validation"
      ]
    },
    {
      "metadata": {
        "id": "Cb-VhMn2ct9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nfolds = 10\n",
        "folds = KFold(n_splits=nfolds, shuffle=True, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kc8ydOamYXac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sample"
      ]
    },
    {
      "metadata": {
        "id": "x6LOn_vKfOF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# params = {'num_leaves': 50,\n",
        "#           'min_data_in_leaf': 30,\n",
        "#           'objective': 'regression',\n",
        "#           'max_depth': 6,\n",
        "#           'learning_rate': 0.1,\n",
        "#           \"min_child_samples\": 100,\n",
        "#           \"boosting\": \"gbdt\",\n",
        "#           \"feature_fraction\": 0.8,\n",
        "#           \"bagging_freq\": 1,\n",
        "#           \"bagging_fraction\": 0.7,\n",
        "#           \"bagging_seed\": 11,\n",
        "#           \"metric\": 'rmse',\n",
        "#           \"lambda_l1\": 0.1,\n",
        "#           \"verbosity\": -1,\n",
        "#           \"device\":'cpu',\n",
        "#           \"n_jobs\":4,\n",
        "#           \"n_estimators\":941}\n",
        "\n",
        "\n",
        "params = {'num_leaves': 80,\n",
        "         'min_data_in_leaf': 10,\n",
        "         'objective': 'regression',\n",
        "         'max_depth': 7,\n",
        "         'learning_rate': 0.005,\n",
        "         \"min_child_samples\": 100,\n",
        "         \"boosting\": \"gbdt\",\n",
        "         \"feature_fraction\": 0.8,\n",
        "         \"bagging_freq\": 1,\n",
        "         \"bagging_fraction\": 0.7,\n",
        "         \"bagging_seed\": 11,\n",
        "         \"metric\": 'rmse',\n",
        "         \"lambda_l1\": 0.1,\n",
        "         \"verbosity\": -1,\n",
        "         \"n_jobs\":8,\n",
        "         \"n_estimators\":941}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2UTsTr6JfQJz",
        "colab_type": "code",
        "outputId": "4d57e90e-af34-4de4-a4e7-47c0d78ef8b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        }
      },
      "cell_type": "code",
      "source": [
        "feature_importance_df = np.zeros((xtrain.shape[1], nfolds))\n",
        "mvalid = np.zeros(len(xtrain))\n",
        "mfull = np.zeros(len(xtest))\n",
        "\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(xtrain.values, ytrain.values)):\n",
        "    print('----')\n",
        "    print(\"fold n°{}\".format(fold_))\n",
        "\n",
        "    x0, y0 = xtrain.iloc[trn_idx], ytrain.iloc[trn_idx]\n",
        "    x1, y1 = xtrain.iloc[val_idx], ytrain.iloc[val_idx]\n",
        "\n",
        "    trn_data = lgb.Dataset(x0, label=y0)\n",
        "    val_data = lgb.Dataset(x1, label=y1)\n",
        "\n",
        "    num_round = 10000\n",
        "    clf = lgb.train(params,\n",
        "                    trn_data,\n",
        "                    num_round,\n",
        "                    valid_sets=[trn_data, val_data],\n",
        "                    verbose_eval=500,\n",
        "                    early_stopping_rounds=150)\n",
        "    \n",
        "    mvalid[val_idx] = clf.predict(x1, num_iteration=clf.best_iteration)\n",
        "\n",
        "    feature_importance_df[:, fold_] = clf.feature_importance()\n",
        "\n",
        "    mfull += clf.predict(xtest,\n",
        "                         num_iteration=clf.best_iteration) / folds.n_splits\n",
        "    \n",
        "    \n",
        "np.sqrt(mean_squared_error(mfull.astype(int), ytest.astype(int)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            "fold n°0\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.97346\tvalid_1's rmse: 3.00919\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.40454\tvalid_1's rmse: 2.98727\n",
            "----\n",
            "fold n°1\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.96027\tvalid_1's rmse: 3.35494\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.36978\tvalid_1's rmse: 3.32063\n",
            "----\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.96716\tvalid_1's rmse: 3.08877\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.39434\tvalid_1's rmse: 3.05962\n",
            "----\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.95053\tvalid_1's rmse: 3.30398\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.37984\tvalid_1's rmse: 3.22539\n",
            "----\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.92492\tvalid_1's rmse: 3.35701\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.31596\tvalid_1's rmse: 3.25499\n",
            "----\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.98711\tvalid_1's rmse: 3.05443\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.40768\tvalid_1's rmse: 3.00008\n",
            "----\n",
            "fold n°6\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.96314\tvalid_1's rmse: 3.33109\n",
            "Early stopping, best iteration is:\n",
            "[672]\ttraining's rmse: 1.67548\tvalid_1's rmse: 3.31314\n",
            "----\n",
            "fold n°7\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.95736\tvalid_1's rmse: 3.26768\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.3824\tvalid_1's rmse: 3.20635\n",
            "----\n",
            "fold n°8\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.96981\tvalid_1's rmse: 3.17998\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.3913\tvalid_1's rmse: 3.11078\n",
            "----\n",
            "fold n°9\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 1.96197\tvalid_1's rmse: 3.08675\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[941]\ttraining's rmse: 1.37162\tvalid_1's rmse: 3.03328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0064682521775197"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "-49GtC8OYgwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Traing with feature level"
      ]
    },
    {
      "metadata": {
        "id": "7eE9VZDf8B5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_lgbm(xtrain, ytrain, xtest, ytest, params):\n",
        "    feature_importance_df = np.zeros((xtrain.shape[1], nfolds))\n",
        "    mvalid = np.zeros(len(xtrain))\n",
        "    mfull = np.zeros(len(xtest))\n",
        "    models = []\n",
        "\n",
        "\n",
        "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(xtrain.values, ytrain.values)):\n",
        "        print('----')\n",
        "        print(\"fold n°{}\".format(fold_))\n",
        "\n",
        "        x0, y0 = xtrain.iloc[trn_idx], ytrain.iloc[trn_idx]\n",
        "        x1, y1 = xtrain.iloc[val_idx], ytrain.iloc[val_idx]\n",
        "\n",
        "        trn_data = lgb.Dataset(x0, label=y0)\n",
        "        val_data = lgb.Dataset(x1, label=y1)\n",
        "\n",
        "        num_round = 10000\n",
        "        clf = lgb.train(params,\n",
        "                        trn_data,\n",
        "                        num_round,\n",
        "                        valid_sets=[trn_data, val_data],\n",
        "                        verbose_eval=500,\n",
        "                        early_stopping_rounds=150)\n",
        "\n",
        "        mvalid[val_idx] = clf.predict(x1, num_iteration=clf.best_iteration)\n",
        "\n",
        "        feature_importance_df[:, fold_] = clf.feature_importance()\n",
        "\n",
        "        mfull += clf.predict(xtest,\n",
        "                             num_iteration=clf.best_iteration) / folds.n_splits\n",
        "        \n",
        "        models.append(clf)\n",
        "\n",
        "\n",
        "    test_error=np.sqrt(mean_squared_error(mfull.astype(int), ytest.astype(int)))\n",
        "    print()\n",
        "    print('rmse:', test_error)\n",
        "    return models, test_error, feature_importance_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EaUCpaxxigat",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### All features"
      ]
    },
    {
      "metadata": {
        "id": "dRevX3MYToQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params_all = {\n",
        "    # objective and metric\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": 'rmse',\n",
        "    \"boosting\": \"gbdt\",\n",
        "\n",
        "    # for the Leaf-wise (Best-first) Tree\n",
        "    \"num_leaves\": 100, \n",
        "    # smaller than 2^(max_depth), This is the main parameter to control the complexity of the tree model. With larger can get higher accuracy \n",
        "    \"min_data_in_leaf\": 30, # Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting.\n",
        "    \"max_depth\": 7, # limit the tree depth explicitly.\n",
        "\n",
        "    # For Faster Speed\n",
        "    \"bagging_fraction\": 0.7,\n",
        "    \"bagging_freq\": 1,\n",
        "#     \"max_bin\": 5, # more small more faster\n",
        "    \"bagging_seed\": 11,\n",
        "\n",
        "    # For Better Accuracy\n",
        "    \"max_bin\": 20, # lager but slower\n",
        "    \"learning_rate\": 0.005,\n",
        "\n",
        "    # deal with over fitting\n",
        "      # Use small max_bin\n",
        "      # Use small num_leaves\n",
        "      # Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
        "      # Use bagging by set bagging_fraction and bagging_freq\n",
        "      # Use feature sub-sampling by set feature_fraction\n",
        "      # Use bigger training data\n",
        "      # Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
        "      # Try max_depth to avoid growing deep tree\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"lambda_l1\": 0.9,\n",
        "\n",
        "    \"min_child_samples\": 100,\n",
        "\n",
        "    # other\n",
        "    \"n_estimators\": 1500,\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\":-1,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4a87i6vic9C",
        "colab_type": "code",
        "outputId": "ecf8979d-2f91-4583-cecf-ceb19e5c5c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "cell_type": "code",
      "source": [
        "models, feature_importance_df = train_lgbm(xtrain, ytrain, xtest, ytest, params_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            "fold n°0\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.48445\tvalid_1's rmse: 2.97748\n",
            "[1000]\ttraining's rmse: 1.94242\tvalid_1's rmse: 2.93612\n",
            "Early stopping, best iteration is:\n",
            "[892]\ttraining's rmse: 2.03945\tvalid_1's rmse: 2.92963\n",
            "----\n",
            "fold n°1\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.46543\tvalid_1's rmse: 3.34328\n",
            "[1000]\ttraining's rmse: 1.92384\tvalid_1's rmse: 3.28782\n",
            "Early stopping, best iteration is:\n",
            "[999]\ttraining's rmse: 1.92488\tvalid_1's rmse: 3.28764\n",
            "----\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.48863\tvalid_1's rmse: 3.07938\n",
            "Early stopping, best iteration is:\n",
            "[775]\ttraining's rmse: 2.15276\tvalid_1's rmse: 3.03324\n",
            "----\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.47913\tvalid_1's rmse: 3.30472\n",
            "[1000]\ttraining's rmse: 1.93591\tvalid_1's rmse: 3.2104\n",
            "Early stopping, best iteration is:\n",
            "[977]\ttraining's rmse: 1.95541\tvalid_1's rmse: 3.20935\n",
            "----\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.45948\tvalid_1's rmse: 3.37598\n",
            "[1000]\ttraining's rmse: 1.92\tvalid_1's rmse: 3.26417\n",
            "[1500]\ttraining's rmse: 1.5485\tvalid_1's rmse: 3.2498\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 1.5485\tvalid_1's rmse: 3.2498\n",
            "----\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.49989\tvalid_1's rmse: 3.06889\n",
            "[1000]\ttraining's rmse: 1.94654\tvalid_1's rmse: 3.01451\n",
            "Early stopping, best iteration is:\n",
            "[876]\ttraining's rmse: 2.05567\tvalid_1's rmse: 3.00995\n",
            "----\n",
            "fold n°6\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.46493\tvalid_1's rmse: 3.28848\n",
            "[1000]\ttraining's rmse: 1.9124\tvalid_1's rmse: 3.26107\n",
            "Early stopping, best iteration is:\n",
            "[884]\ttraining's rmse: 2.01704\tvalid_1's rmse: 3.25667\n",
            "----\n",
            "fold n°7\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.46586\tvalid_1's rmse: 3.28687\n",
            "[1000]\ttraining's rmse: 1.93353\tvalid_1's rmse: 3.19697\n",
            "Early stopping, best iteration is:\n",
            "[1012]\ttraining's rmse: 1.92326\tvalid_1's rmse: 3.19654\n",
            "----\n",
            "fold n°8\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.49338\tvalid_1's rmse: 3.16801\n",
            "[1000]\ttraining's rmse: 1.94856\tvalid_1's rmse: 3.08893\n",
            "Early stopping, best iteration is:\n",
            "[1343]\ttraining's rmse: 1.68282\tvalid_1's rmse: 3.07405\n",
            "----\n",
            "fold n°9\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.48517\tvalid_1's rmse: 3.07673\n",
            "[1000]\ttraining's rmse: 1.93127\tvalid_1's rmse: 3.01102\n",
            "Early stopping, best iteration is:\n",
            "[1285]\ttraining's rmse: 1.71459\tvalid_1's rmse: 3.00547\n",
            "2.9957740505856183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agg6BTWYt2Gf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Params search"
      ]
    },
    {
      "metadata": {
        "id": "in2taBBM7vEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params_all = {\n",
        "    # objective and metric\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": 'rmse',\n",
        "    \"boosting\": \"gbdt\",\n",
        "\n",
        "    # for the Leaf-wise (Best-first) Tree\n",
        "    \"num_leaves\": 100, \n",
        "    # smaller than 2^(max_depth), This is the main parameter to control the complexity of the tree model. With larger can get higher accuracy \n",
        "    \"min_data_in_leaf\": 30, # Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting.\n",
        "    \"max_depth\": 7, # limit the tree depth explicitly.\n",
        "\n",
        "    # For Faster Speed\n",
        "    \"bagging_fraction\": 0.7,\n",
        "    \"bagging_freq\": 1,\n",
        "#     \"max_bin\": 5, # more small more faster\n",
        "    \"bagging_seed\": 11,\n",
        "\n",
        "    # For Better Accuracy\n",
        "    \"max_bin\": 20, # lager but slower\n",
        "    \"learning_rate\": 0.005,\n",
        "\n",
        "    # deal with over fitting\n",
        "      # Use small max_bin\n",
        "      # Use small num_leaves\n",
        "      # Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
        "      # Use bagging by set bagging_fraction and bagging_freq\n",
        "      # Use feature sub-sampling by set feature_fraction\n",
        "      # Use bigger training data\n",
        "      # Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
        "      # Try max_depth to avoid growing deep tree\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"lambda_l1\": 0.9,\n",
        "\n",
        "    \"min_child_samples\": 100,\n",
        "\n",
        "    # other\n",
        "    \"n_estimators\": 1500,\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\":-1,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBY8wwaZt9Qs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params_range_dict = {# for the Leaf-wise (Best-first) Tree\n",
        "    \"num_leaves\": np.arange(10,160,10, dtype=int), \n",
        "    # smaller than 2^(max_depth), This is the main parameter to control the complexity of the tree model. With larger can get higher accuracy \n",
        "#     \"min_data_in_leaf\": np.arange(10,55,5), # Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting.\n",
        "    \"max_depth\": np.arange(4,13,1, dtype=int), # limit the tree depth explicitly.\n",
        "\n",
        "#     # For Faster Speed\n",
        "#     \"bagging_fraction\": np.arange(0.5, 1,0.1),\n",
        "\n",
        "#     # For Better Accuracy\n",
        "    \"max_bin\": np.arange(10,60,5, dtype=int), # lager but slower\n",
        "\n",
        "#     \"feature_fraction\": np.arange(0.3, 1, 0.1),\n",
        "#     \"lambda_l1\": np.arange(0.01, 1, 0.05),\n",
        "\n",
        "#     \"min_child_samples\": np.arange(70, 110,10),\n",
        "}\n",
        "\n",
        "# params_range_dict ={\"num_leaves\": np.arange(10,30,10, dtype=int)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CieZ_qULvrEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def search_params(params_range_dict):\n",
        "#   count = 0\n",
        "#   train_dict={}\n",
        "#   error_dict={}\n",
        "#   params_name=list(params_range_dict.keys())\n",
        "#   params_values=[]\n",
        "#   for name in params_name:\n",
        "#     params_values.append(params_range_dict[name])\n",
        "\n",
        "#   c_params = list(itertools.product(*params_values))\n",
        "#   idx = [i for i in range(0, len(c_params))]\n",
        "\n",
        "#   params_df = pd.DataFrame(c_params, index=idx, columns=params_name)\n",
        "#   print('Got combinations:',len(params_df))\n",
        "#   for row in range(0,len(params_df)):\n",
        "#     print('*********************************Iteration: %s****************************************************'% str(count))\n",
        "#     each_params=params_df.iloc[row].to_dict()\n",
        "#     print('current params:', each_params)\n",
        "#     params_all.update(each_params)\n",
        "#     models, test_error, feature_importance_df = train_lgbm(xtrain, ytrain, xtest, ytest, params_all)\n",
        "#     train_dict[count]={'models':models,'rmse':test_error,'feature_importance':feature_importance_df}\n",
        "#     error_dict[count]=test_error\n",
        "#     count+=1\n",
        "#   best_=sorted(error_dict.items(), key=lambda item: item[1])[0]\n",
        "#   print('Best:', best_)\n",
        "#   return train_dict, best_\n",
        "def search_params(params_range_dict):\n",
        "    best_rmse = np.inf\n",
        "    count = 0\n",
        "    train_dict = {}\n",
        "    params_name = list(params_range_dict.keys())\n",
        "    params_values = []\n",
        "    for name in params_name:\n",
        "        params_values.append(params_range_dict[name])\n",
        "\n",
        "    c_params = list(itertools.product(*params_values))\n",
        "    idx = [i for i in range(0, len(c_params))]\n",
        "\n",
        "    params_df = pd.DataFrame(c_params, index=idx, columns=params_name)\n",
        "    print('Iteration number:', len(params_df))\n",
        "    for row in range(0, len(params_df)):\n",
        "        print('*********************************Iteration: %s****************************************************' % str(count))\n",
        "        starttime=time.time()\n",
        "        each_params = params_df.iloc[row].to_dict()\n",
        "        print('Current params:', each_params)\n",
        "        params_all.update(each_params)\n",
        "        models, test_error, feature_importance_df = train_lgbm(\n",
        "            xtrain, ytrain, xtest, ytest, params_all)\n",
        "        \n",
        "        print(\"Used %s seconds\" % (time.time() - starttime))\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        if test_error < best_rmse:\n",
        "            best_rmse = test_error\n",
        "            train_dict = {'models': models, 'rmse': test_error,\n",
        "                          'feature_importance': feature_importance_df, 'params': each_params}\n",
        "        print('Current best:', best_rmse, 'Params:', train_dict['params'])\n",
        "        print()\n",
        "\n",
        "    return train_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRij3SsI81iv",
        "colab_type": "code",
        "outputId": "89c3057b-50bd-49e5-b060-4741d7d3b387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3196
        }
      },
      "cell_type": "code",
      "source": [
        "train_dict = search_params(params_range_dict)\n",
        "train_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration number: 1350\n",
            "*********************************Iteration: 0****************************************************\n",
            "Current params: {'num_leaves': 10, 'max_depth': 4, 'max_bin': 10}\n",
            "----\n",
            "fold n°0\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.02959\tvalid_1's rmse: 3.00725\n",
            "[1000]\ttraining's rmse: 2.68004\tvalid_1's rmse: 2.92797\n",
            "Early stopping, best iteration is:\n",
            "[1163]\ttraining's rmse: 2.60326\tvalid_1's rmse: 2.92404\n",
            "----\n",
            "fold n°1\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.99913\tvalid_1's rmse: 3.42519\n",
            "[1000]\ttraining's rmse: 2.64916\tvalid_1's rmse: 3.32422\n",
            "Early stopping, best iteration is:\n",
            "[1095]\ttraining's rmse: 2.60376\tvalid_1's rmse: 3.32106\n",
            "----\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.02003\tvalid_1's rmse: 3.1766\n",
            "[1000]\ttraining's rmse: 2.67453\tvalid_1's rmse: 3.09104\n",
            "Early stopping, best iteration is:\n",
            "[989]\ttraining's rmse: 2.68028\tvalid_1's rmse: 3.09022\n",
            "----\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.01032\tvalid_1's rmse: 3.3804\n",
            "[1000]\ttraining's rmse: 2.66746\tvalid_1's rmse: 3.22906\n",
            "Early stopping, best iteration is:\n",
            "[1301]\ttraining's rmse: 2.52968\tvalid_1's rmse: 3.20914\n",
            "----\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.9908\tvalid_1's rmse: 3.47162\n",
            "[1000]\ttraining's rmse: 2.64938\tvalid_1's rmse: 3.30028\n",
            "[1500]\ttraining's rmse: 2.42989\tvalid_1's rmse: 3.27324\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 2.42989\tvalid_1's rmse: 3.27324\n",
            "----\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.02953\tvalid_1's rmse: 3.13956\n",
            "[1000]\ttraining's rmse: 2.67261\tvalid_1's rmse: 3.02468\n",
            "Early stopping, best iteration is:\n",
            "[1162]\ttraining's rmse: 2.59337\tvalid_1's rmse: 3.02092\n",
            "----\n",
            "fold n°6\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.99718\tvalid_1's rmse: 3.36393\n",
            "[1000]\ttraining's rmse: 2.64045\tvalid_1's rmse: 3.29792\n",
            "Early stopping, best iteration is:\n",
            "[1300]\ttraining's rmse: 2.50156\tvalid_1's rmse: 3.29234\n",
            "----\n",
            "fold n°7\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.01687\tvalid_1's rmse: 3.2773\n",
            "[1000]\ttraining's rmse: 2.67013\tvalid_1's rmse: 3.13402\n",
            "Early stopping, best iteration is:\n",
            "[1098]\ttraining's rmse: 2.62241\tvalid_1's rmse: 3.12902\n",
            "----\n",
            "fold n°8\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.02473\tvalid_1's rmse: 3.24594\n",
            "[1000]\ttraining's rmse: 2.67899\tvalid_1's rmse: 3.12796\n",
            "[1500]\ttraining's rmse: 2.45657\tvalid_1's rmse: 3.1158\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 2.45657\tvalid_1's rmse: 3.1158\n",
            "----\n",
            "fold n°9\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.02912\tvalid_1's rmse: 3.15985\n",
            "[1000]\ttraining's rmse: 2.67336\tvalid_1's rmse: 3.06412\n",
            "Early stopping, best iteration is:\n",
            "[1074]\ttraining's rmse: 2.63769\tvalid_1's rmse: 3.06193\n",
            "\n",
            "rmse: 3.0413812651491097\n",
            "Used 40.03526020050049 seconds\n",
            "Current best: 3.0413812651491097 Params: {'num_leaves': 10, 'max_depth': 4, 'max_bin': 10}\n",
            "\n",
            "*********************************Iteration: 1****************************************************\n",
            "Current params: {'num_leaves': 10, 'max_depth': 4, 'max_bin': 15}\n",
            "----\n",
            "fold n°0\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.01529\tvalid_1's rmse: 3.00706\n",
            "[1000]\ttraining's rmse: 2.65744\tvalid_1's rmse: 2.91608\n",
            "Early stopping, best iteration is:\n",
            "[1142]\ttraining's rmse: 2.58904\tvalid_1's rmse: 2.91342\n",
            "----\n",
            "fold n°1\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.98514\tvalid_1's rmse: 3.4223\n",
            "[1000]\ttraining's rmse: 2.62725\tvalid_1's rmse: 3.33557\n",
            "Early stopping, best iteration is:\n",
            "[1031]\ttraining's rmse: 2.61199\tvalid_1's rmse: 3.33347\n",
            "----\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.00206\tvalid_1's rmse: 3.17875\n",
            "[1000]\ttraining's rmse: 2.64673\tvalid_1's rmse: 3.10267\n",
            "Early stopping, best iteration is:\n",
            "[1239]\ttraining's rmse: 2.5329\tvalid_1's rmse: 3.09822\n",
            "----\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.99174\tvalid_1's rmse: 3.41007\n",
            "[1000]\ttraining's rmse: 2.64036\tvalid_1's rmse: 3.26614\n",
            "[1500]\ttraining's rmse: 2.41497\tvalid_1's rmse: 3.24798\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 2.41497\tvalid_1's rmse: 3.24798\n",
            "----\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.97664\tvalid_1's rmse: 3.47843\n",
            "[1000]\ttraining's rmse: 2.62712\tvalid_1's rmse: 3.3105\n",
            "[1500]\ttraining's rmse: 2.40483\tvalid_1's rmse: 3.29126\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 2.40483\tvalid_1's rmse: 3.29126\n",
            "----\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.01731\tvalid_1's rmse: 3.11669\n",
            "[1000]\ttraining's rmse: 2.6594\tvalid_1's rmse: 3.00579\n",
            "Early stopping, best iteration is:\n",
            "[1124]\ttraining's rmse: 2.59768\tvalid_1's rmse: 3.00179\n",
            "----\n",
            "fold n°6\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.98258\tvalid_1's rmse: 3.39045\n",
            "[1000]\ttraining's rmse: 2.6203\tvalid_1's rmse: 3.33322\n",
            "Early stopping, best iteration is:\n",
            "[894]\ttraining's rmse: 2.67926\tvalid_1's rmse: 3.32753\n",
            "----\n",
            "fold n°7\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.99863\tvalid_1's rmse: 3.33144\n",
            "[1000]\ttraining's rmse: 2.64377\tvalid_1's rmse: 3.18622\n",
            "Early stopping, best iteration is:\n",
            "[1259]\ttraining's rmse: 2.52109\tvalid_1's rmse: 3.17902\n",
            "----\n",
            "fold n°8\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.00527\tvalid_1's rmse: 3.27327\n",
            "[1000]\ttraining's rmse: 2.65003\tvalid_1's rmse: 3.14889\n",
            "[1500]\ttraining's rmse: 2.42361\tvalid_1's rmse: 3.13437\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 2.42361\tvalid_1's rmse: 3.13437\n",
            "----\n",
            "fold n°9\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.00947\tvalid_1's rmse: 3.16015\n",
            "[1000]\ttraining's rmse: 2.64638\tvalid_1's rmse: 3.08622\n",
            "Early stopping, best iteration is:\n",
            "[1322]\ttraining's rmse: 2.49401\tvalid_1's rmse: 3.0811\n",
            "\n",
            "rmse: 3.0358221647876897\n",
            "Used 41.98555397987366 seconds\n",
            "Current best: 3.0358221647876897 Params: {'num_leaves': 10, 'max_depth': 4, 'max_bin': 15}\n",
            "\n",
            "*********************************Iteration: 2****************************************************\n",
            "Current params: {'num_leaves': 10, 'max_depth': 4, 'max_bin': 20}\n",
            "----\n",
            "fold n°0\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 3.00183\tvalid_1's rmse: 3.00187\n",
            "[1000]\ttraining's rmse: 2.64043\tvalid_1's rmse: 2.91959\n",
            "Early stopping, best iteration is:\n",
            "[1000]\ttraining's rmse: 2.64043\tvalid_1's rmse: 2.91959\n",
            "----\n",
            "fold n°1\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.96976\tvalid_1's rmse: 3.42699\n",
            "[1000]\ttraining's rmse: 2.60901\tvalid_1's rmse: 3.33627\n",
            "Early stopping, best iteration is:\n",
            "[982]\ttraining's rmse: 2.61878\tvalid_1's rmse: 3.33564\n",
            "----\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.9934\tvalid_1's rmse: 3.16769\n",
            "[1000]\ttraining's rmse: 2.63526\tvalid_1's rmse: 3.0938\n",
            "Early stopping, best iteration is:\n",
            "[1159]\ttraining's rmse: 2.55651\tvalid_1's rmse: 3.08644\n",
            "----\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.98731\tvalid_1's rmse: 3.39083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "88blZTl6aQCm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature selection"
      ]
    },
    {
      "metadata": {
        "id": "S-oy6ISCCyWe",
        "colab_type": "code",
        "outputId": "1a801540-37ea-4425-98ce-3e20a6b78215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "ximp = pd.DataFrame()\n",
        "ximp['feature'] = xtrain.columns\n",
        "ximp['importance'] = feature_importance_df.mean(axis=1)\n",
        "\n",
        "stddf=x.describe().loc[['std']]\n",
        "stddf.loc['variance'] = stddf.loc['std']**2\n",
        "\n",
        "stddf.drop(['企业编号'],axis=1,inplace=True)\n",
        "\n",
        "variance=stddf.T\n",
        "variance['feature']=variance.index\n",
        "variance.drop(['std'],axis=1,inplace=True)\n",
        "features=ximp.merge(variance, how='left',on='feature')\n",
        "\n",
        "features=features.sort_values(by=['importance'], ascending=False)\n",
        "features['rank']=(np.arange(1,len(features)+1))\n",
        "features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f9dc97911d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mximp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mximp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mximp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'importance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_importance_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstddf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'feature_importance_df' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fqwf70dq2BX6",
        "colab_type": "code",
        "outputId": "542f648e-0b73-4980-d6b5-d90af85d3769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "(features.loc[(features['rank']>=features['importance'])]).iloc[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature          投资流出_max\n",
              "importance          186.8\n",
              "variance      4.01759e+22\n",
              "rank                  187\n",
              "Name: 186, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "R7aithj46lyw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dr=features[['rank','importance']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCgbTd158Ctq",
        "colab_type": "code",
        "outputId": "a72c9030-08ad-4992-9fcb-a695122264a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(features['rank'],features['importance'],color=\"red\",linewidth=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f40d1eb2278>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9oAAAI/CAYAAACI6aHCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "rIBzmCbcBS57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "feature_filter=list(features.loc[(features['rank']<=187)]['feature'])\n",
        "feature_filter.append('企业编号')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdw5yok1Egho",
        "colab_type": "code",
        "outputId": "89bbe2d9-f0e7-4418-ae43-b6e53b0482d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2057
        }
      },
      "cell_type": "code",
      "source": [
        "x_filter=x[list(feature_filter)]\n",
        "x_filter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>软著数量</th>\n",
              "      <th>发明授权</th>\n",
              "      <th>实用新型</th>\n",
              "      <th>外观设计</th>\n",
              "      <th>发明公布</th>\n",
              "      <th>商标数量</th>\n",
              "      <th>资质证书数量</th>\n",
              "      <th>行业大类_C</th>\n",
              "      <th>作品著作数量</th>\n",
              "      <th>成立年数</th>\n",
              "      <th>...</th>\n",
              "      <th>摊薄总资产收益率(%)_min</th>\n",
              "      <th>净利率(%)_mean</th>\n",
              "      <th>资产:资产总计(元)_min</th>\n",
              "      <th>每股经营现金流(元)_max</th>\n",
              "      <th>负债:负债合计(元)_max</th>\n",
              "      <th>财务费用(元)_max</th>\n",
              "      <th>投资流出_mean</th>\n",
              "      <th>加权净资产收益率(%)_mean</th>\n",
              "      <th>投资流出_max</th>\n",
              "      <th>企业编号</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.343078</td>\n",
              "      <td>0.418085</td>\n",
              "      <td>-1.6600</td>\n",
              "      <td>10.002016</td>\n",
              "      <td>1.950526e+08</td>\n",
              "      <td>3.173491e+12</td>\n",
              "      <td>0.097644</td>\n",
              "      <td>4.768984e+12</td>\n",
              "      <td>1001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0190</td>\n",
              "      <td>0.227522</td>\n",
              "      <td>16.642118</td>\n",
              "      <td>1.7009</td>\n",
              "      <td>140.656871</td>\n",
              "      <td>7.248514e+07</td>\n",
              "      <td>1.865108e+09</td>\n",
              "      <td>0.078367</td>\n",
              "      <td>4.084310e+09</td>\n",
              "      <td>1002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>955.0</td>\n",
              "      <td>469.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>442.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0078</td>\n",
              "      <td>0.015733</td>\n",
              "      <td>-24.609220</td>\n",
              "      <td>0.3714</td>\n",
              "      <td>28.259997</td>\n",
              "      <td>-3.211562e+06</td>\n",
              "      <td>3.602564e+09</td>\n",
              "      <td>0.030111</td>\n",
              "      <td>5.645571e+09</td>\n",
              "      <td>1003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0139</td>\n",
              "      <td>0.261311</td>\n",
              "      <td>-2.714605</td>\n",
              "      <td>1.1968</td>\n",
              "      <td>11.256137</td>\n",
              "      <td>1.415579e+08</td>\n",
              "      <td>1.273136e+09</td>\n",
              "      <td>0.071667</td>\n",
              "      <td>2.377073e+09</td>\n",
              "      <td>1004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.073122</td>\n",
              "      <td>19.680974</td>\n",
              "      <td>0.6137</td>\n",
              "      <td>40.738386</td>\n",
              "      <td>9.116558e+08</td>\n",
              "      <td>5.613110e+09</td>\n",
              "      <td>0.034333</td>\n",
              "      <td>8.845762e+09</td>\n",
              "      <td>1005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0304</td>\n",
              "      <td>0.460989</td>\n",
              "      <td>1.124474</td>\n",
              "      <td>2.1350</td>\n",
              "      <td>10.928766</td>\n",
              "      <td>-4.530856e+07</td>\n",
              "      <td>1.944693e+09</td>\n",
              "      <td>0.093338</td>\n",
              "      <td>2.977211e+09</td>\n",
              "      <td>1006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>269.0</td>\n",
              "      <td>836.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>962.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.031044</td>\n",
              "      <td>1.595949</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>3.305182</td>\n",
              "      <td>1.854352e+09</td>\n",
              "      <td>7.738014e+08</td>\n",
              "      <td>0.020033</td>\n",
              "      <td>1.836114e+09</td>\n",
              "      <td>1007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.045956</td>\n",
              "      <td>-0.253721</td>\n",
              "      <td>2.0730</td>\n",
              "      <td>44.984566</td>\n",
              "      <td>9.405739e+09</td>\n",
              "      <td>1.951729e+10</td>\n",
              "      <td>0.041922</td>\n",
              "      <td>3.948164e+10</td>\n",
              "      <td>1008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0185</td>\n",
              "      <td>0.376922</td>\n",
              "      <td>5.913169</td>\n",
              "      <td>1.1352</td>\n",
              "      <td>12.033064</td>\n",
              "      <td>4.234151e+07</td>\n",
              "      <td>8.979506e+08</td>\n",
              "      <td>0.075056</td>\n",
              "      <td>1.649485e+09</td>\n",
              "      <td>1009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>0.295967</td>\n",
              "      <td>5.947799</td>\n",
              "      <td>13.0930</td>\n",
              "      <td>7.455572</td>\n",
              "      <td>1.950526e+08</td>\n",
              "      <td>6.078777e+11</td>\n",
              "      <td>0.086044</td>\n",
              "      <td>8.751980e+11</td>\n",
              "      <td>1010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>13.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.376067</td>\n",
              "      <td>0.105311</td>\n",
              "      <td>28.1994</td>\n",
              "      <td>9.551637</td>\n",
              "      <td>1.950526e+08</td>\n",
              "      <td>1.234871e+12</td>\n",
              "      <td>0.107322</td>\n",
              "      <td>2.333150e+12</td>\n",
              "      <td>1011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.116011</td>\n",
              "      <td>1.538395</td>\n",
              "      <td>0.4499</td>\n",
              "      <td>10.311345</td>\n",
              "      <td>2.455945e+08</td>\n",
              "      <td>1.250935e+09</td>\n",
              "      <td>0.026400</td>\n",
              "      <td>2.971502e+09</td>\n",
              "      <td>1012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0123</td>\n",
              "      <td>0.247644</td>\n",
              "      <td>1.609951</td>\n",
              "      <td>0.4147</td>\n",
              "      <td>51.154162</td>\n",
              "      <td>1.200000e+09</td>\n",
              "      <td>4.428438e+09</td>\n",
              "      <td>0.073488</td>\n",
              "      <td>9.012871e+09</td>\n",
              "      <td>1013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>41.0</td>\n",
              "      <td>3455.0</td>\n",
              "      <td>3362.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5986.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0131</td>\n",
              "      <td>0.059333</td>\n",
              "      <td>-4.183480</td>\n",
              "      <td>1.6504</td>\n",
              "      <td>38.113075</td>\n",
              "      <td>3.696282e+09</td>\n",
              "      <td>6.682425e+10</td>\n",
              "      <td>0.063667</td>\n",
              "      <td>1.214489e+11</td>\n",
              "      <td>1014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.225900</td>\n",
              "      <td>-6.301801</td>\n",
              "      <td>1.4998</td>\n",
              "      <td>11.264645</td>\n",
              "      <td>1.429081e+09</td>\n",
              "      <td>1.825937e+09</td>\n",
              "      <td>0.057689</td>\n",
              "      <td>4.928868e+09</td>\n",
              "      <td>1015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.069611</td>\n",
              "      <td>7.266406</td>\n",
              "      <td>1.8741</td>\n",
              "      <td>63.503308</td>\n",
              "      <td>1.651151e+09</td>\n",
              "      <td>4.700150e+09</td>\n",
              "      <td>0.042267</td>\n",
              "      <td>7.223896e+09</td>\n",
              "      <td>1016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.0</td>\n",
              "      <td>533.0</td>\n",
              "      <td>1111.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0100</td>\n",
              "      <td>0.033822</td>\n",
              "      <td>-11.779552</td>\n",
              "      <td>0.4575</td>\n",
              "      <td>52.913568</td>\n",
              "      <td>4.250021e+08</td>\n",
              "      <td>4.933698e+09</td>\n",
              "      <td>0.050800</td>\n",
              "      <td>9.978566e+09</td>\n",
              "      <td>1017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.118367</td>\n",
              "      <td>1.795026</td>\n",
              "      <td>0.9816</td>\n",
              "      <td>11.345474</td>\n",
              "      <td>1.054245e+09</td>\n",
              "      <td>5.564119e+09</td>\n",
              "      <td>0.057011</td>\n",
              "      <td>1.167705e+10</td>\n",
              "      <td>1018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.178378</td>\n",
              "      <td>0.312999</td>\n",
              "      <td>0.5210</td>\n",
              "      <td>3.624472</td>\n",
              "      <td>3.563796e+09</td>\n",
              "      <td>4.953960e+09</td>\n",
              "      <td>0.035087</td>\n",
              "      <td>8.646731e+09</td>\n",
              "      <td>1019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>0.120544</td>\n",
              "      <td>-16.434866</td>\n",
              "      <td>3.6964</td>\n",
              "      <td>8.700274</td>\n",
              "      <td>8.643271e+08</td>\n",
              "      <td>3.816339e+09</td>\n",
              "      <td>0.030833</td>\n",
              "      <td>7.459432e+09</td>\n",
              "      <td>1020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>16.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>0.032456</td>\n",
              "      <td>2.470474</td>\n",
              "      <td>2.2440</td>\n",
              "      <td>7.051320</td>\n",
              "      <td>5.043795e+09</td>\n",
              "      <td>9.403101e+09</td>\n",
              "      <td>0.026756</td>\n",
              "      <td>1.589879e+10</td>\n",
              "      <td>1021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>473.0</td>\n",
              "      <td>31757.0</td>\n",
              "      <td>13506.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>54460.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.033867</td>\n",
              "      <td>3.833106</td>\n",
              "      <td>1.7720</td>\n",
              "      <td>26.742233</td>\n",
              "      <td>1.790000e+09</td>\n",
              "      <td>1.048883e+11</td>\n",
              "      <td>0.051244</td>\n",
              "      <td>2.122550e+11</td>\n",
              "      <td>1022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0091</td>\n",
              "      <td>0.061544</td>\n",
              "      <td>6.540942</td>\n",
              "      <td>2.6877</td>\n",
              "      <td>14.538616</td>\n",
              "      <td>4.531000e+09</td>\n",
              "      <td>9.443000e+09</td>\n",
              "      <td>0.089700</td>\n",
              "      <td>1.471500e+10</td>\n",
              "      <td>1023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5726.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.287600</td>\n",
              "      <td>-2.790136</td>\n",
              "      <td>3.6834</td>\n",
              "      <td>18.691223</td>\n",
              "      <td>1.950526e+08</td>\n",
              "      <td>2.588962e+09</td>\n",
              "      <td>0.046133</td>\n",
              "      <td>8.237262e+09</td>\n",
              "      <td>1024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>28.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>491.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>399.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.075022</td>\n",
              "      <td>-5.389130</td>\n",
              "      <td>1.1202</td>\n",
              "      <td>14.733829</td>\n",
              "      <td>1.319034e+09</td>\n",
              "      <td>2.711804e+09</td>\n",
              "      <td>0.068444</td>\n",
              "      <td>3.790269e+09</td>\n",
              "      <td>1025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.397211</td>\n",
              "      <td>-2.425561</td>\n",
              "      <td>0.6644</td>\n",
              "      <td>-8.551507</td>\n",
              "      <td>2.475784e+08</td>\n",
              "      <td>1.280229e+08</td>\n",
              "      <td>0.058489</td>\n",
              "      <td>2.432205e+08</td>\n",
              "      <td>1026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.269878</td>\n",
              "      <td>-0.888342</td>\n",
              "      <td>0.5996</td>\n",
              "      <td>7.216245</td>\n",
              "      <td>1.960340e+08</td>\n",
              "      <td>4.516032e+08</td>\n",
              "      <td>0.068333</td>\n",
              "      <td>7.894388e+08</td>\n",
              "      <td>1027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>672.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.343256</td>\n",
              "      <td>4.192262</td>\n",
              "      <td>-0.2244</td>\n",
              "      <td>12.015176</td>\n",
              "      <td>1.950526e+08</td>\n",
              "      <td>4.891173e+11</td>\n",
              "      <td>0.115667</td>\n",
              "      <td>9.402170e+11</td>\n",
              "      <td>1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>114.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.310522</td>\n",
              "      <td>1.610629</td>\n",
              "      <td>0.8513</td>\n",
              "      <td>17.123643</td>\n",
              "      <td>-3.386832e+07</td>\n",
              "      <td>1.493001e+09</td>\n",
              "      <td>0.038989</td>\n",
              "      <td>3.690957e+09</td>\n",
              "      <td>1029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.034244</td>\n",
              "      <td>6.090482</td>\n",
              "      <td>1.0409</td>\n",
              "      <td>16.497925</td>\n",
              "      <td>2.524911e+06</td>\n",
              "      <td>7.113212e+07</td>\n",
              "      <td>0.035933</td>\n",
              "      <td>1.214377e+08</td>\n",
              "      <td>1030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2926</th>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.088422</td>\n",
              "      <td>2.571048</td>\n",
              "      <td>0.0958</td>\n",
              "      <td>21.851343</td>\n",
              "      <td>1.726454e+07</td>\n",
              "      <td>2.013233e+08</td>\n",
              "      <td>0.033011</td>\n",
              "      <td>3.158353e+08</td>\n",
              "      <td>3971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2927</th>\n",
              "      <td>99.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.261522</td>\n",
              "      <td>16.104507</td>\n",
              "      <td>0.4218</td>\n",
              "      <td>87.667493</td>\n",
              "      <td>1.335586e+08</td>\n",
              "      <td>1.130094e+09</td>\n",
              "      <td>0.069856</td>\n",
              "      <td>1.600277e+09</td>\n",
              "      <td>3972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>132.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0141</td>\n",
              "      <td>0.288089</td>\n",
              "      <td>3.315692</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>59.308452</td>\n",
              "      <td>-7.445996e+06</td>\n",
              "      <td>2.400668e+08</td>\n",
              "      <td>0.063333</td>\n",
              "      <td>3.724754e+08</td>\n",
              "      <td>3973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.040378</td>\n",
              "      <td>-2.137221</td>\n",
              "      <td>0.0320</td>\n",
              "      <td>437.628218</td>\n",
              "      <td>5.135045e+07</td>\n",
              "      <td>1.039586e+09</td>\n",
              "      <td>0.057878</td>\n",
              "      <td>1.638435e+09</td>\n",
              "      <td>3974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.062900</td>\n",
              "      <td>0.462298</td>\n",
              "      <td>0.0673</td>\n",
              "      <td>23.086434</td>\n",
              "      <td>1.594296e+08</td>\n",
              "      <td>6.932499e+08</td>\n",
              "      <td>0.023222</td>\n",
              "      <td>1.379615e+09</td>\n",
              "      <td>3975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.085800</td>\n",
              "      <td>6.557141</td>\n",
              "      <td>0.4752</td>\n",
              "      <td>49.678906</td>\n",
              "      <td>1.047309e+07</td>\n",
              "      <td>2.501218e+08</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>9.341377e+08</td>\n",
              "      <td>3976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>110.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>299.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>0.082922</td>\n",
              "      <td>8.898435</td>\n",
              "      <td>0.6943</td>\n",
              "      <td>57.519606</td>\n",
              "      <td>-2.791496e+06</td>\n",
              "      <td>3.965852e+08</td>\n",
              "      <td>0.039956</td>\n",
              "      <td>1.039372e+09</td>\n",
              "      <td>3977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0095</td>\n",
              "      <td>0.041944</td>\n",
              "      <td>-42.002349</td>\n",
              "      <td>0.4632</td>\n",
              "      <td>-6.867631</td>\n",
              "      <td>1.304262e+07</td>\n",
              "      <td>3.481251e+08</td>\n",
              "      <td>0.001478</td>\n",
              "      <td>1.135967e+09</td>\n",
              "      <td>3978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>4.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.120078</td>\n",
              "      <td>9.565781</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>45.057680</td>\n",
              "      <td>2.706936e+07</td>\n",
              "      <td>3.529968e+08</td>\n",
              "      <td>0.030589</td>\n",
              "      <td>8.758300e+08</td>\n",
              "      <td>3979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>44.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0185</td>\n",
              "      <td>-0.079856</td>\n",
              "      <td>-0.649030</td>\n",
              "      <td>0.5753</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>7.880000e+08</td>\n",
              "      <td>5.694037e+07</td>\n",
              "      <td>0.033928</td>\n",
              "      <td>3.782922e+08</td>\n",
              "      <td>3980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.090844</td>\n",
              "      <td>1.459007</td>\n",
              "      <td>0.8930</td>\n",
              "      <td>936.993972</td>\n",
              "      <td>2.037027e+07</td>\n",
              "      <td>2.287450e+08</td>\n",
              "      <td>0.042111</td>\n",
              "      <td>4.786617e+08</td>\n",
              "      <td>3981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>24.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>0.170433</td>\n",
              "      <td>5.972774</td>\n",
              "      <td>0.4982</td>\n",
              "      <td>15.685484</td>\n",
              "      <td>1.721222e+07</td>\n",
              "      <td>1.214496e+08</td>\n",
              "      <td>0.039111</td>\n",
              "      <td>3.163436e+08</td>\n",
              "      <td>3982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2938</th>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.154144</td>\n",
              "      <td>1.550290</td>\n",
              "      <td>0.9435</td>\n",
              "      <td>27.854552</td>\n",
              "      <td>1.154916e+05</td>\n",
              "      <td>1.182834e+09</td>\n",
              "      <td>0.050156</td>\n",
              "      <td>1.739012e+09</td>\n",
              "      <td>3983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2939</th>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.130333</td>\n",
              "      <td>-1.914326</td>\n",
              "      <td>0.8067</td>\n",
              "      <td>85.187352</td>\n",
              "      <td>1.489627e+07</td>\n",
              "      <td>8.032020e+08</td>\n",
              "      <td>0.030356</td>\n",
              "      <td>1.216925e+09</td>\n",
              "      <td>3984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2940</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>0.073678</td>\n",
              "      <td>1.155783</td>\n",
              "      <td>1.5513</td>\n",
              "      <td>12.200000</td>\n",
              "      <td>2.965914e+07</td>\n",
              "      <td>3.815716e+08</td>\n",
              "      <td>0.049639</td>\n",
              "      <td>1.693645e+09</td>\n",
              "      <td>3985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2941</th>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>0.065322</td>\n",
              "      <td>41.268929</td>\n",
              "      <td>0.1011</td>\n",
              "      <td>131.108048</td>\n",
              "      <td>4.163336e+08</td>\n",
              "      <td>1.862127e+09</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>2.992938e+09</td>\n",
              "      <td>3986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2942</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0066</td>\n",
              "      <td>0.028467</td>\n",
              "      <td>8.345329</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>82.633067</td>\n",
              "      <td>3.802652e+07</td>\n",
              "      <td>2.679528e+08</td>\n",
              "      <td>0.018000</td>\n",
              "      <td>4.858648e+08</td>\n",
              "      <td>3987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>0.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.337578</td>\n",
              "      <td>6.414194</td>\n",
              "      <td>0.3228</td>\n",
              "      <td>141.589601</td>\n",
              "      <td>1.608747e+07</td>\n",
              "      <td>1.627655e+08</td>\n",
              "      <td>0.061456</td>\n",
              "      <td>2.678587e+08</td>\n",
              "      <td>3988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2944</th>\n",
              "      <td>0.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>149.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.079800</td>\n",
              "      <td>-19.393560</td>\n",
              "      <td>0.5558</td>\n",
              "      <td>1227.232966</td>\n",
              "      <td>5.619999e+06</td>\n",
              "      <td>3.759747e+08</td>\n",
              "      <td>0.037744</td>\n",
              "      <td>7.364204e+08</td>\n",
              "      <td>3989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2945</th>\n",
              "      <td>6.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>191.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0072</td>\n",
              "      <td>0.034511</td>\n",
              "      <td>5.335369</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>34.133986</td>\n",
              "      <td>3.074644e+07</td>\n",
              "      <td>1.088185e+08</td>\n",
              "      <td>0.039056</td>\n",
              "      <td>2.693882e+08</td>\n",
              "      <td>3990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2946</th>\n",
              "      <td>12.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.126322</td>\n",
              "      <td>-2.189683</td>\n",
              "      <td>0.3698</td>\n",
              "      <td>10.127182</td>\n",
              "      <td>1.646219e+07</td>\n",
              "      <td>6.528727e+07</td>\n",
              "      <td>0.040344</td>\n",
              "      <td>1.399366e+08</td>\n",
              "      <td>3991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2947</th>\n",
              "      <td>248.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>268.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>295.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.148933</td>\n",
              "      <td>16.335164</td>\n",
              "      <td>0.2638</td>\n",
              "      <td>37.203804</td>\n",
              "      <td>6.737929e+07</td>\n",
              "      <td>3.511297e+08</td>\n",
              "      <td>0.084244</td>\n",
              "      <td>4.679416e+08</td>\n",
              "      <td>3992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0212</td>\n",
              "      <td>0.198078</td>\n",
              "      <td>6.919109</td>\n",
              "      <td>0.5727</td>\n",
              "      <td>20.859398</td>\n",
              "      <td>-1.676494e+06</td>\n",
              "      <td>1.182901e+09</td>\n",
              "      <td>0.081556</td>\n",
              "      <td>2.010647e+09</td>\n",
              "      <td>3993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2949</th>\n",
              "      <td>75.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.020289</td>\n",
              "      <td>4.807802</td>\n",
              "      <td>0.4333</td>\n",
              "      <td>30.530041</td>\n",
              "      <td>5.803224e+06</td>\n",
              "      <td>1.551612e+08</td>\n",
              "      <td>0.027078</td>\n",
              "      <td>3.246963e+08</td>\n",
              "      <td>3994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>293.0</td>\n",
              "      <td>249.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>370.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.085278</td>\n",
              "      <td>-2.748838</td>\n",
              "      <td>0.1948</td>\n",
              "      <td>-1.742190</td>\n",
              "      <td>2.380159e+07</td>\n",
              "      <td>1.794760e+08</td>\n",
              "      <td>0.036167</td>\n",
              "      <td>3.153207e+08</td>\n",
              "      <td>3995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2951</th>\n",
              "      <td>12.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>0.038044</td>\n",
              "      <td>49.933655</td>\n",
              "      <td>0.8107</td>\n",
              "      <td>86.367394</td>\n",
              "      <td>1.716183e+08</td>\n",
              "      <td>1.217992e+09</td>\n",
              "      <td>0.110711</td>\n",
              "      <td>2.092210e+09</td>\n",
              "      <td>3996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0233</td>\n",
              "      <td>0.254156</td>\n",
              "      <td>37.814766</td>\n",
              "      <td>-0.0381</td>\n",
              "      <td>96.238117</td>\n",
              "      <td>3.265119e+07</td>\n",
              "      <td>3.835764e+07</td>\n",
              "      <td>0.135078</td>\n",
              "      <td>1.117046e+08</td>\n",
              "      <td>3997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2953</th>\n",
              "      <td>66.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0100</td>\n",
              "      <td>0.058844</td>\n",
              "      <td>0.760473</td>\n",
              "      <td>0.2578</td>\n",
              "      <td>50.128833</td>\n",
              "      <td>1.658854e+06</td>\n",
              "      <td>1.695442e+08</td>\n",
              "      <td>0.022522</td>\n",
              "      <td>3.128856e+08</td>\n",
              "      <td>3998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2954</th>\n",
              "      <td>4.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>0.145544</td>\n",
              "      <td>-3.227312</td>\n",
              "      <td>0.2180</td>\n",
              "      <td>64.763987</td>\n",
              "      <td>3.629995e+07</td>\n",
              "      <td>1.081674e+08</td>\n",
              "      <td>0.035911</td>\n",
              "      <td>2.171958e+08</td>\n",
              "      <td>3999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2955</th>\n",
              "      <td>41.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.033678</td>\n",
              "      <td>-3.985993</td>\n",
              "      <td>0.1185</td>\n",
              "      <td>-8.856762</td>\n",
              "      <td>-1.245577e+06</td>\n",
              "      <td>5.928305e+07</td>\n",
              "      <td>0.006867</td>\n",
              "      <td>1.060296e+08</td>\n",
              "      <td>4000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2956 rows × 188 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       软著数量     发明授权     实用新型   外观设计     发明公布   商标数量  资质证书数量  行业大类_C  作品著作数量  \\\n",
              "0       1.0      0.0      0.0    0.0      4.0  255.0     9.0     0.0     1.0   \n",
              "1       2.0      0.0      1.0    0.0      1.0    5.0     0.0     0.0     0.0   \n",
              "2       2.0     61.0    955.0  469.0    141.0   90.0   442.0     1.0     0.0   \n",
              "3       0.0      0.0      1.0    5.0      1.0    6.0     1.0     0.0     6.0   \n",
              "4       6.0      2.0      6.0    0.0      6.0   17.0     5.0     0.0     0.0   \n",
              "5       2.0      5.0      8.0    2.0     12.0    5.0     6.0     0.0     1.0   \n",
              "6       0.0    269.0    836.0    2.0    962.0    8.0     7.0     1.0     0.0   \n",
              "7       0.0     79.0    218.0    0.0    242.0   20.0     1.0     0.0     0.0   \n",
              "8       0.0      0.0      2.0    0.0      0.0    3.0     0.0     0.0     0.0   \n",
              "9       0.0      2.0      0.0    1.0     16.0  129.0     0.0     0.0     0.0   \n",
              "10     13.0     21.0      7.0    5.0    130.0  199.0     8.0     0.0     3.0   \n",
              "11     15.0      1.0      9.0    1.0      2.0    0.0     0.0     0.0     0.0   \n",
              "12      4.0      6.0     17.0    5.0     25.0   96.0     1.0     0.0     0.0   \n",
              "13     41.0   3455.0   3362.0    0.0   5986.0  126.0     8.0     0.0     0.0   \n",
              "14      0.0      4.0      5.0    2.0      5.0    8.0     0.0     0.0     1.0   \n",
              "15      0.0     27.0     30.0    0.0     44.0    0.0     6.0     0.0     0.0   \n",
              "16      0.0    533.0   1111.0    0.0   1268.0    6.0     3.0     1.0     0.0   \n",
              "17      0.0      0.0      0.0    0.0      0.0    0.0     6.0     0.0     0.0   \n",
              "18     20.0     25.0     68.0    0.0    117.0   24.0     0.0     0.0     4.0   \n",
              "19      0.0      0.0      0.0    0.0      0.0    0.0     0.0     0.0     0.0   \n",
              "20     16.0      2.0     21.0    0.0      7.0    0.0     0.0     0.0     0.0   \n",
              "21    473.0  31757.0  13506.0   62.0  54460.0   45.0    26.0     0.0     0.0   \n",
              "22      1.0      3.0      1.0    0.0     27.0   86.0     1.0     0.0     0.0   \n",
              "23      9.0      0.0      0.0    0.0      0.0   11.0     0.0     0.0  5726.0   \n",
              "24     28.0    266.0    491.0   71.0    399.0   11.0    35.0     1.0     0.0   \n",
              "25      0.0      0.0      0.0    0.0      0.0    0.0     0.0     0.0     0.0   \n",
              "26      0.0      0.0      0.0    0.0      0.0    0.0     0.0     0.0     0.0   \n",
              "27      0.0      0.0      0.0    0.0      0.0  672.0     8.0     0.0     1.0   \n",
              "28    114.0      0.0      0.0    0.0      0.0   11.0     0.0     0.0     0.0   \n",
              "29      0.0      0.0      0.0    0.0      0.0    0.0     0.0     1.0     0.0   \n",
              "...     ...      ...      ...    ...      ...    ...     ...     ...     ...   \n",
              "2926    0.0     15.0      0.0    0.0     24.0    0.0     1.0     1.0     0.0   \n",
              "2927   99.0     20.0      3.0    1.0     43.0    8.0     1.0     0.0     0.0   \n",
              "2928  132.0     26.0     28.0   11.0     51.0   17.0    10.0     0.0     3.0   \n",
              "2929    0.0      0.0      9.0    0.0      0.0   11.0     0.0     1.0     0.0   \n",
              "2930    0.0     32.0     57.0    0.0     70.0    5.0     4.0     1.0     0.0   \n",
              "2931    0.0     15.0     45.0    0.0     35.0    3.0     0.0     0.0     0.0   \n",
              "2932  110.0    134.0     32.0   15.0    299.0   33.0     0.0     0.0     1.0   \n",
              "2933    0.0      1.0      0.0    0.0      1.0   35.0     1.0     0.0     0.0   \n",
              "2934    4.0     22.0     56.0    1.0     37.0   17.0     5.0     1.0     0.0   \n",
              "2935   44.0      2.0      0.0    0.0      2.0    5.0     0.0     0.0     0.0   \n",
              "2936    0.0      8.0      8.0   53.0     10.0    0.0     5.0     1.0     2.0   \n",
              "2937   24.0     13.0     62.0   20.0     25.0   68.0   225.0     0.0     1.0   \n",
              "2938    0.0     17.0      0.0    0.0     27.0   22.0     0.0     1.0     1.0   \n",
              "2939    4.0    130.0    229.0   14.0    212.0   39.0     0.0     1.0     0.0   \n",
              "2940    0.0      2.0     22.0    0.0     12.0    1.0     2.0     1.0     0.0   \n",
              "2941    0.0     45.0    117.0    5.0    118.0   31.0     6.0     0.0     4.0   \n",
              "2942    0.0      0.0     18.0    0.0      6.0   16.0     4.0     1.0     0.0   \n",
              "2943    0.0    167.0      2.0    2.0    310.0   85.0     2.0     1.0     0.0   \n",
              "2944    0.0    111.0      0.0    0.0    149.0    3.0     3.0     1.0     0.0   \n",
              "2945    6.0     42.0    191.0   15.0     96.0   14.0    20.0     0.0     0.0   \n",
              "2946   12.0     44.0    117.0   37.0     80.0   21.0    38.0     0.0     0.0   \n",
              "2947  248.0    199.0    268.0    3.0    295.0   16.0    30.0     1.0     0.0   \n",
              "2948    0.0     36.0      0.0    6.0     67.0  103.0     0.0     1.0     4.0   \n",
              "2949   75.0     59.0     73.0   40.0    138.0   53.0    24.0     1.0     0.0   \n",
              "2950  293.0    249.0    240.0  110.0    370.0    1.0    13.0     0.0     0.0   \n",
              "2951   12.0     38.0    114.0   37.0    184.0   84.0    11.0     1.0     0.0   \n",
              "2952    4.0      5.0     46.0    0.0     12.0    2.0     3.0     0.0     0.0   \n",
              "2953   66.0     24.0     13.0   10.0     48.0  132.0    15.0     0.0     0.0   \n",
              "2954    4.0     26.0     57.0    2.0     38.0    3.0   103.0     1.0     0.0   \n",
              "2955   41.0     19.0     14.0  122.0     40.0   34.0    22.0     1.0     0.0   \n",
              "\n",
              "      成立年数  ...   摊薄总资产收益率(%)_min  净利率(%)_mean  资产:资产总计(元)_min  \\\n",
              "0     14.0  ...            0.0024     0.343078        0.418085   \n",
              "1     13.0  ...            0.0190     0.227522       16.642118   \n",
              "2     24.0  ...           -0.0078     0.015733      -24.609220   \n",
              "3     17.0  ...            0.0139     0.261311       -2.714605   \n",
              "4     20.0  ...            0.0023     0.073122       19.680974   \n",
              "5     19.0  ...            0.0304     0.460989        1.124474   \n",
              "6     19.0  ...            0.0006     0.031044        1.595949   \n",
              "7     16.0  ...            0.0028     0.045956       -0.253721   \n",
              "8     23.0  ...            0.0185     0.376922        5.913169   \n",
              "9     14.0  ...            0.0018     0.295967        5.947799   \n",
              "10    17.0  ...            0.0024     0.376067        0.105311   \n",
              "11    20.0  ...            0.0064     0.116011        1.538395   \n",
              "12    20.0  ...            0.0123     0.247644        1.609951   \n",
              "13    16.0  ...            0.0131     0.059333       -4.183480   \n",
              "14    22.0  ...            0.0054     0.225900       -6.301801   \n",
              "15    23.0  ...            0.0030     0.069611        7.266406   \n",
              "16    30.0  ...           -0.0100     0.033822      -11.779552   \n",
              "17    19.0  ...            0.0057     0.118367        1.795026   \n",
              "18    24.0  ...            0.0025     0.178378        0.312999   \n",
              "19    18.0  ...           -0.0021     0.120544      -16.434866   \n",
              "20    17.0  ...           -0.0007     0.032456        2.470474   \n",
              "21    18.0  ...            0.0146     0.033867        3.833106   \n",
              "22    20.0  ...            0.0091     0.061544        6.540942   \n",
              "23    27.0  ...            0.0040     0.287600       -2.790136   \n",
              "24    20.0  ...            0.0027     0.075022       -5.389130   \n",
              "25    19.0  ...            0.0136     0.397211       -2.425561   \n",
              "26    18.0  ...            0.0156     0.269878       -0.888342   \n",
              "27    10.0  ...            0.0034     0.343256        4.192262   \n",
              "28    22.0  ...            0.0111     0.310522        1.610629   \n",
              "29    22.0  ...            0.0031     0.034244        6.090482   \n",
              "...    ...  ...               ...          ...             ...   \n",
              "2926  20.0  ...            0.0050     0.088422        2.571048   \n",
              "2927  20.0  ...            0.0095     0.261522       16.104507   \n",
              "2928  20.0  ...            0.0141     0.288089        3.315692   \n",
              "2929  20.0  ...            0.0101     0.040378       -2.137221   \n",
              "2930  20.0  ...            0.0017     0.062900        0.462298   \n",
              "2931  20.0  ...            0.0048     0.085800        6.557141   \n",
              "2932  20.0  ...           -0.0005     0.082922        8.898435   \n",
              "2933  20.0  ...           -0.0095     0.041944      -42.002349   \n",
              "2934  20.0  ...            0.0056     0.120078        9.565781   \n",
              "2935  20.0  ...           -0.0185    -0.079856       -0.649030   \n",
              "2936  20.0  ...            0.0047     0.090844        1.459007   \n",
              "2937  20.0  ...            0.0078     0.170433        5.972774   \n",
              "2938  20.0  ...            0.0187     0.154144        1.550290   \n",
              "2939  20.0  ...            0.0081     0.130333       -1.914326   \n",
              "2940  20.0  ...           -0.0060     0.073678        1.155783   \n",
              "2941  20.0  ...           -0.0035     0.065322       41.268929   \n",
              "2942  20.0  ...           -0.0066     0.028467        8.345329   \n",
              "2943  20.0  ...            0.0176     0.337578        6.414194   \n",
              "2944  20.0  ...            0.0053     0.079800      -19.393560   \n",
              "2945  20.0  ...           -0.0072     0.034511        5.335369   \n",
              "2946  20.0  ...            0.0019     0.126322       -2.189683   \n",
              "2947  20.0  ...            0.0027     0.148933       16.335164   \n",
              "2948  20.0  ...            0.0212     0.198078        6.919109   \n",
              "2949  20.0  ...            0.0012     0.020289        4.807802   \n",
              "2950  20.0  ...            0.0041     0.085278       -2.748838   \n",
              "2951  20.0  ...            0.0070     0.038044       49.933655   \n",
              "2952  20.0  ...            0.0233     0.254156       37.814766   \n",
              "2953  20.0  ...           -0.0100     0.058844        0.760473   \n",
              "2954  20.0  ...            0.0078     0.145544       -3.227312   \n",
              "2955  20.0  ...            0.0031     0.033678       -3.985993   \n",
              "\n",
              "      每股经营现金流(元)_max  负债:负债合计(元)_max   财务费用(元)_max     投资流出_mean  \\\n",
              "0            -1.6600       10.002016  1.950526e+08  3.173491e+12   \n",
              "1             1.7009      140.656871  7.248514e+07  1.865108e+09   \n",
              "2             0.3714       28.259997 -3.211562e+06  3.602564e+09   \n",
              "3             1.1968       11.256137  1.415579e+08  1.273136e+09   \n",
              "4             0.6137       40.738386  9.116558e+08  5.613110e+09   \n",
              "5             2.1350       10.928766 -4.530856e+07  1.944693e+09   \n",
              "6             0.1984        3.305182  1.854352e+09  7.738014e+08   \n",
              "7             2.0730       44.984566  9.405739e+09  1.951729e+10   \n",
              "8             1.1352       12.033064  4.234151e+07  8.979506e+08   \n",
              "9            13.0930        7.455572  1.950526e+08  6.078777e+11   \n",
              "10           28.1994        9.551637  1.950526e+08  1.234871e+12   \n",
              "11            0.4499       10.311345  2.455945e+08  1.250935e+09   \n",
              "12            0.4147       51.154162  1.200000e+09  4.428438e+09   \n",
              "13            1.6504       38.113075  3.696282e+09  6.682425e+10   \n",
              "14            1.4998       11.264645  1.429081e+09  1.825937e+09   \n",
              "15            1.8741       63.503308  1.651151e+09  4.700150e+09   \n",
              "16            0.4575       52.913568  4.250021e+08  4.933698e+09   \n",
              "17            0.9816       11.345474  1.054245e+09  5.564119e+09   \n",
              "18            0.5210        3.624472  3.563796e+09  4.953960e+09   \n",
              "19            3.6964        8.700274  8.643271e+08  3.816339e+09   \n",
              "20            2.2440        7.051320  5.043795e+09  9.403101e+09   \n",
              "21            1.7720       26.742233  1.790000e+09  1.048883e+11   \n",
              "22            2.6877       14.538616  4.531000e+09  9.443000e+09   \n",
              "23            3.6834       18.691223  1.950526e+08  2.588962e+09   \n",
              "24            1.1202       14.733829  1.319034e+09  2.711804e+09   \n",
              "25            0.6644       -8.551507  2.475784e+08  1.280229e+08   \n",
              "26            0.5996        7.216245  1.960340e+08  4.516032e+08   \n",
              "27           -0.2244       12.015176  1.950526e+08  4.891173e+11   \n",
              "28            0.8513       17.123643 -3.386832e+07  1.493001e+09   \n",
              "29            1.0409       16.497925  2.524911e+06  7.113212e+07   \n",
              "...              ...             ...           ...           ...   \n",
              "2926          0.0958       21.851343  1.726454e+07  2.013233e+08   \n",
              "2927          0.4218       87.667493  1.335586e+08  1.130094e+09   \n",
              "2928          0.6042       59.308452 -7.445996e+06  2.400668e+08   \n",
              "2929          0.0320      437.628218  5.135045e+07  1.039586e+09   \n",
              "2930          0.0673       23.086434  1.594296e+08  6.932499e+08   \n",
              "2931          0.4752       49.678906  1.047309e+07  2.501218e+08   \n",
              "2932          0.6943       57.519606 -2.791496e+06  3.965852e+08   \n",
              "2933          0.4632       -6.867631  1.304262e+07  3.481251e+08   \n",
              "2934          0.1442       45.057680  2.706936e+07  3.529968e+08   \n",
              "2935          0.5753      183.000000  7.880000e+08  5.694037e+07   \n",
              "2936          0.8930      936.993972  2.037027e+07  2.287450e+08   \n",
              "2937          0.4982       15.685484  1.721222e+07  1.214496e+08   \n",
              "2938          0.9435       27.854552  1.154916e+05  1.182834e+09   \n",
              "2939          0.8067       85.187352  1.489627e+07  8.032020e+08   \n",
              "2940          1.5513       12.200000  2.965914e+07  3.815716e+08   \n",
              "2941          0.1011      131.108048  4.163336e+08  1.862127e+09   \n",
              "2942          0.2587       82.633067  3.802652e+07  2.679528e+08   \n",
              "2943          0.3228      141.589601  1.608747e+07  1.627655e+08   \n",
              "2944          0.5558     1227.232966  5.619999e+06  3.759747e+08   \n",
              "2945          0.0328       34.133986  3.074644e+07  1.088185e+08   \n",
              "2946          0.3698       10.127182  1.646219e+07  6.528727e+07   \n",
              "2947          0.2638       37.203804  6.737929e+07  3.511297e+08   \n",
              "2948          0.5727       20.859398 -1.676494e+06  1.182901e+09   \n",
              "2949          0.4333       30.530041  5.803224e+06  1.551612e+08   \n",
              "2950          0.1948       -1.742190  2.380159e+07  1.794760e+08   \n",
              "2951          0.8107       86.367394  1.716183e+08  1.217992e+09   \n",
              "2952         -0.0381       96.238117  3.265119e+07  3.835764e+07   \n",
              "2953          0.2578       50.128833  1.658854e+06  1.695442e+08   \n",
              "2954          0.2180       64.763987  3.629995e+07  1.081674e+08   \n",
              "2955          0.1185       -8.856762 -1.245577e+06  5.928305e+07   \n",
              "\n",
              "      加权净资产收益率(%)_mean      投资流出_max  企业编号  \n",
              "0             0.097644  4.768984e+12  1001  \n",
              "1             0.078367  4.084310e+09  1002  \n",
              "2             0.030111  5.645571e+09  1003  \n",
              "3             0.071667  2.377073e+09  1004  \n",
              "4             0.034333  8.845762e+09  1005  \n",
              "5             0.093338  2.977211e+09  1006  \n",
              "6             0.020033  1.836114e+09  1007  \n",
              "7             0.041922  3.948164e+10  1008  \n",
              "8             0.075056  1.649485e+09  1009  \n",
              "9             0.086044  8.751980e+11  1010  \n",
              "10            0.107322  2.333150e+12  1011  \n",
              "11            0.026400  2.971502e+09  1012  \n",
              "12            0.073488  9.012871e+09  1013  \n",
              "13            0.063667  1.214489e+11  1014  \n",
              "14            0.057689  4.928868e+09  1015  \n",
              "15            0.042267  7.223896e+09  1016  \n",
              "16            0.050800  9.978566e+09  1017  \n",
              "17            0.057011  1.167705e+10  1018  \n",
              "18            0.035087  8.646731e+09  1019  \n",
              "19            0.030833  7.459432e+09  1020  \n",
              "20            0.026756  1.589879e+10  1021  \n",
              "21            0.051244  2.122550e+11  1022  \n",
              "22            0.089700  1.471500e+10  1023  \n",
              "23            0.046133  8.237262e+09  1024  \n",
              "24            0.068444  3.790269e+09  1025  \n",
              "25            0.058489  2.432205e+08  1026  \n",
              "26            0.068333  7.894388e+08  1027  \n",
              "27            0.115667  9.402170e+11  1028  \n",
              "28            0.038989  3.690957e+09  1029  \n",
              "29            0.035933  1.214377e+08  1030  \n",
              "...                ...           ...   ...  \n",
              "2926          0.033011  3.158353e+08  3971  \n",
              "2927          0.069856  1.600277e+09  3972  \n",
              "2928          0.063333  3.724754e+08  3973  \n",
              "2929          0.057878  1.638435e+09  3974  \n",
              "2930          0.023222  1.379615e+09  3975  \n",
              "2931          0.043244  9.341377e+08  3976  \n",
              "2932          0.039956  1.039372e+09  3977  \n",
              "2933          0.001478  1.135967e+09  3978  \n",
              "2934          0.030589  8.758300e+08  3979  \n",
              "2935          0.033928  3.782922e+08  3980  \n",
              "2936          0.042111  4.786617e+08  3981  \n",
              "2937          0.039111  3.163436e+08  3982  \n",
              "2938          0.050156  1.739012e+09  3983  \n",
              "2939          0.030356  1.216925e+09  3984  \n",
              "2940          0.049639  1.693645e+09  3985  \n",
              "2941          0.062500  2.992938e+09  3986  \n",
              "2942          0.018000  4.858648e+08  3987  \n",
              "2943          0.061456  2.678587e+08  3988  \n",
              "2944          0.037744  7.364204e+08  3989  \n",
              "2945          0.039056  2.693882e+08  3990  \n",
              "2946          0.040344  1.399366e+08  3991  \n",
              "2947          0.084244  4.679416e+08  3992  \n",
              "2948          0.081556  2.010647e+09  3993  \n",
              "2949          0.027078  3.246963e+08  3994  \n",
              "2950          0.036167  3.153207e+08  3995  \n",
              "2951          0.110711  2.092210e+09  3996  \n",
              "2952          0.135078  1.117046e+08  3997  \n",
              "2953          0.022522  3.128856e+08  3998  \n",
              "2954          0.035911  2.171958e+08  3999  \n",
              "2955          0.006867  1.060296e+08  4000  \n",
              "\n",
              "[2956 rows x 188 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "metadata": {
        "id": "X7NXNvqZEnML",
        "colab_type": "code",
        "outputId": "8b597a31-3d40-47d6-976f-94ad6de363ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "xtrain_filter, xtest_filter, ytrain, ytest = train_test_split(\n",
        "    x_filter, y, test_size=0.2, random_state=0)\n",
        "ytrain_id = ytrain['企业编号']\n",
        "ytrain = ytrain['企业总评分']\n",
        "ytest_id = ytest['企业编号']\n",
        "ytest = ytest['企业总评分']\n",
        "print(xtrain_filter.shape, xtest_filter.shape, ytrain.shape, ytest.shape)\n",
        "\n",
        "id_train = xtrain_filter['企业编号']\n",
        "id_test = xtest_filter['企业编号']\n",
        "xtrain_filter.drop(['企业编号'], axis=1, inplace=True)\n",
        "xtest_filter.drop(['企业编号'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2364, 188) (592, 188) (2364,) (592,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CrpG_GhpLimc",
        "colab_type": "code",
        "outputId": "7d1fdd85-867d-49cc-dcc8-78ba63d8e7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        }
      },
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    # objective and metric\n",
        "    \"objective\": \"regression\",\n",
        "    \"metric\": 'rmse',\n",
        "    \"boosting\": \"gbdt\",\n",
        "\n",
        "    # for the Leaf-wise (Best-first) Tree\n",
        "    \"num_leaves\": 120, \n",
        "    # smaller than 2^(max_depth), This is the main parameter to control the complexity of the tree model. With larger can get higher accuracy \n",
        "    \"min_data_in_leaf\": 20, # Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting.\n",
        "    \"max_depth\": 7, # limit the tree depth explicitly.\n",
        "\n",
        "    # For Faster Speed\n",
        "    \"bagging_fraction\": 0.7,\n",
        "    \"bagging_freq\": 1,\n",
        "#     \"max_bin\": 5, # more small more faster\n",
        "    \"bagging_seed\": 11,\n",
        "\n",
        "    # For Better Accuracy\n",
        "    \"max_bin\": 30, # lager but slower\n",
        "    \"learning_rate\": 0.005,\n",
        "\n",
        "    # deal with over fitting\n",
        "      # Use small max_bin\n",
        "      # Use small num_leaves\n",
        "      # Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
        "      # Use bagging by set bagging_fraction and bagging_freq\n",
        "      # Use feature sub-sampling by set feature_fraction\n",
        "      # Use bigger training data\n",
        "      # Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
        "      # Try max_depth to avoid growing deep tree\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"lambda_l1\": 0.1,\n",
        "\n",
        "    \"min_child_samples\": 100,\n",
        "\n",
        "    # other\n",
        "    \"n_estimators\": 1500, # num_itertations的别名，默认是100.也就是循环次数，或者叫树的数目\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\":8,\n",
        "}\n",
        "models, feature_importance_df = train_lgbm(xtrain_filter, ytrain, xtest_filter, ytest, params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            "fold n°0\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.27283\tvalid_1's rmse: 2.97459\n",
            "[1000]\ttraining's rmse: 1.69521\tvalid_1's rmse: 2.92697\n",
            "Early stopping, best iteration is:\n",
            "[1124]\ttraining's rmse: 1.5867\tvalid_1's rmse: 2.92481\n",
            "----\n",
            "fold n°1\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.25937\tvalid_1's rmse: 3.29271\n",
            "Early stopping, best iteration is:\n",
            "[836]\ttraining's rmse: 1.81141\tvalid_1's rmse: 3.23493\n",
            "----\n",
            "fold n°2\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.28011\tvalid_1's rmse: 3.06421\n",
            "Early stopping, best iteration is:\n",
            "[849]\ttraining's rmse: 1.83199\tvalid_1's rmse: 3.02137\n",
            "----\n",
            "fold n°3\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.25778\tvalid_1's rmse: 3.29559\n",
            "[1000]\ttraining's rmse: 1.67113\tvalid_1's rmse: 3.20162\n",
            "Early stopping, best iteration is:\n",
            "[1177]\ttraining's rmse: 1.52116\tvalid_1's rmse: 3.19817\n",
            "----\n",
            "fold n°4\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.25002\tvalid_1's rmse: 3.36715\n",
            "[1000]\ttraining's rmse: 1.64198\tvalid_1's rmse: 3.26415\n",
            "[1500]\ttraining's rmse: 1.23966\tvalid_1's rmse: 3.25353\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 1.23966\tvalid_1's rmse: 3.25353\n",
            "----\n",
            "fold n°5\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.28644\tvalid_1's rmse: 3.04083\n",
            "[1000]\ttraining's rmse: 1.68045\tvalid_1's rmse: 2.98176\n",
            "Early stopping, best iteration is:\n",
            "[974]\ttraining's rmse: 1.70391\tvalid_1's rmse: 2.97861\n",
            "----\n",
            "fold n°6\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.25944\tvalid_1's rmse: 3.30784\n",
            "Early stopping, best iteration is:\n",
            "[779]\ttraining's rmse: 1.87313\tvalid_1's rmse: 3.28215\n",
            "----\n",
            "fold n°7\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.26518\tvalid_1's rmse: 3.25296\n",
            "[1000]\ttraining's rmse: 1.66654\tvalid_1's rmse: 3.18055\n",
            "[1500]\ttraining's rmse: 1.28138\tvalid_1's rmse: 3.17535\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1500]\ttraining's rmse: 1.28138\tvalid_1's rmse: 3.17535\n",
            "----\n",
            "fold n°8\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.28004\tvalid_1's rmse: 3.11519\n",
            "[1000]\ttraining's rmse: 1.67279\tvalid_1's rmse: 3.02835\n",
            "Early stopping, best iteration is:\n",
            "[1288]\ttraining's rmse: 1.43211\tvalid_1's rmse: 3.02182\n",
            "----\n",
            "fold n°9\n",
            "Training until validation scores don't improve for 150 rounds.\n",
            "[500]\ttraining's rmse: 2.28058\tvalid_1's rmse: 3.07391\n",
            "[1000]\ttraining's rmse: 1.68106\tvalid_1's rmse: 3.03412\n",
            "Early stopping, best iteration is:\n",
            "[919]\ttraining's rmse: 1.7562\tvalid_1's rmse: 3.03143\n",
            "2.9890001039848797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "trYwECPbGJjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}